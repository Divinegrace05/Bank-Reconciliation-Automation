{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "418dfbcb",
   "metadata": {},
   "source": [
    "# EQUITY KE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94a60a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Narrative'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Narrative'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m narrative_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRTGS NALA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Ensure the narrative is exactly matched, stripping any extra whitespace from the bank statement 'Narrative' column\u001b[39;00m\n\u001b[1;32m     49\u001b[0m equity_tz_bank_df_recon \u001b[38;5;241m=\u001b[39m equity_ke_df_recon[\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mequity_ke_df_recon\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNarrative\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m narrative_filter\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m ]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\u001b[39;00m\n\u001b[1;32m     54\u001b[0m total_internal_credits \u001b[38;5;241m=\u001b[39m equity_hex_df_recon[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Narrative'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets ---\n",
    "# Load equityHex_May.csv (Internal Records)\n",
    "equity_hex_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/equityHex_May.csv')\n",
    "\n",
    "# Load EquityKe_May.xlsx - Sheet0.csv (Bank Statements)\n",
    "# We identified that the actual headers are in row index 8 (9th row)\n",
    "equity_ke_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/EquityKe_May - Sheet0.csv', header=8)\n",
    "\n",
    "# --- 2. Preprocessing for equity_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "equity_hex_df.columns = equity_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency: 'TRANSFER_DATE' to 'Date', 'AMOUNT' to 'Amount'\n",
    "equity_hex_df = equity_hex_df.rename(columns={'TRANSFER_DATE': 'Date', 'AMOUNT': 'Amount'})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "equity_hex_df['Date'] = pd.to_datetime(equity_hex_df['Date'])\n",
    "\n",
    "# Select relevant columns for reconciliation. \n",
    "equity_hex_df_recon = equity_hex_df[['Date', 'Amount']]\n",
    "\n",
    "# --- 3. Preprocessing for equity_ke_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "equity_ke_df.columns = equity_ke_df.columns.str.strip()\n",
    "\n",
    "# Rename 'Transaction Date' to 'Date' for consistency\n",
    "equity_ke_df = equity_ke_df.rename(columns={'Transaction Date': 'Date'})\n",
    "\n",
    "# Convert 'Date' to datetime objects (assuming day-first format like '02-05-2025')\n",
    "equity_ke_df['Date'] = pd.to_datetime(equity_ke_df['Date'], dayfirst=True)\n",
    "\n",
    "# Convert 'Credit' to numeric, coercing errors to NaN, then fill NaN with 0\n",
    "equity_ke_df['Credit'] = pd.to_numeric(equity_ke_df['Credit'], errors='coerce').fillna(0)\n",
    "\n",
    "# For reconciliation, consider only credit values from the bank statements; ignore debits.\n",
    "equity_ke_df['Amount'] = equity_ke_df['Credit']\n",
    "\n",
    "# Select relevant columns for reconciliation.\n",
    "equity_ke_df_recon = equity_ke_df[['Date', 'Amount']]\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "equity_ke_df_recon = equity_ke_df_recon[equity_ke_df_recon['Amount'] > 0]\n",
    "\n",
    "# --- NEW FILTER: Filter bank records by 'Narrative' ---\n",
    "narrative_filter = 'RTGS NALA'\n",
    "# Ensure the narrative is exactly matched, stripping any extra whitespace from the bank statement 'Narrative' column\n",
    "equity_tz_bank_df_recon = equity_ke_df_recon[\n",
    "    equity_ke_df_recon['Narrative'].astype(str).str.strip() == narrative_filter.strip()\n",
    "].copy()\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = equity_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = equity_ke_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Amount:{total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Credit Amount: {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy : {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "equity_hex_df_recon = equity_hex_df_recon.copy()\n",
    "equity_ke_df_recon = equity_ke_df_recon.copy()\n",
    "\n",
    "equity_hex_df_recon['Amount_Rounded'] = equity_hex_df_recon['Amount'].round(2)\n",
    "equity_ke_df_recon['Amount_Rounded'] = equity_ke_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date' and 'Amount_Rounded' to find matched and unmatched transactions\n",
    "reconciled_df = pd.merge(\n",
    "    equity_hex_df_recon.assign(Source_Internal='Internal'), # Add a source column for internal records\n",
    "    equity_ke_df_recon.assign(Source_Bank='Bank'),         # Add a source column for bank statements\n",
    "    on=['Date', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions (present in both internal and bank)\n",
    "matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Identify unmatched internal transactions (present in internal but not in bank)\n",
    "unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()]\n",
    "\n",
    "# Identify unmatched bank transactions (present in bank but not in internal)\n",
    "unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f089590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Records for Reconciliation: 19\n",
      "Total Bank Statement Credit Records for Reconciliation: 499\n",
      "Matched Credit Transactions: 18\n",
      "Unmatched Internal Credit Records: 1\n",
      "Unmatched Bank Credit Records: 481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"\\n--- Matched Credit Transactions (First 5) ---\")\\nprint(matched_transactions.head())\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation: {len(equity_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation: {len(equity_ke_df_recon)}\")\n",
    "print(f\"Matched Credit Transactions: {len(matched_transactions)}\")\n",
    "print(f\"Unmatched Internal Credit Records: {len(unmatched_internal)}\")\n",
    "print(f\"Unmatched Bank Credit Records: {len(unmatched_bank)}\")\n",
    "\n",
    "'''\n",
    "print(\"\\n--- Matched Credit Transactions (First 5) ---\")\n",
    "print(matched_transactions.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c924b1a2",
   "metadata": {},
   "source": [
    "# CELLULANT KE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bb250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Cellulant): 55,368,000.00\n",
      "Total Bank Statement Credit Amount (Cellulant): 50,368,000.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits, Cellulant):5,000,000.00\n",
      "\n",
      "Total Internal Credit Records for Reconciliation: 10\n",
      "Total Bank Statement Credit Records for Reconciliation: 9\n",
      "Matched Credit Transactions: 9\n",
      "Unmatched Internal Credit Records: 1\n",
      "Unmatched Bank Credit Records: 0\n",
      "\n",
      "--- Unmatched Internal Credit Records ---\n",
      "  Date_Internal  Amount_Internal                       Description_Internal  \\\n",
      "0    2025-05-06        5000000.0  fx-deal-quote-2wfaDKa7EJYRv3NHHD0NMJmuFwp   \n",
      "\n",
      "                               TRANSFER_ID  Date_Match  Amount_Rounded  \\\n",
      "0  fx-transfer-2y31sVA0iQ5iffnK38eIRZ7TLxo  2025-05-06       5000000.0   \n",
      "\n",
      "  Source_Internal Date_Bank  Amount_Bank Description_Bank  ID Source_Bank  \n",
      "0        Internal       NaT          NaN              NaN NaN         NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_46529/1634025871.py:47: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  cellulant_ke_df['Date'] = pd.to_datetime(cellulant_ke_df['Date'], infer_datetime_format=True)\n",
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_46529/1634025871.py:47: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cellulant_ke_df['Date'] = pd.to_datetime(cellulant_ke_df['Date'], infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets with correct headers ---\n",
    "# Load CellulantKeHex_May.csv (Internal Records)\n",
    "cellulant_hex_df = pd.read_csv('/Users/gracegitau/Downloads/CellulantKeHex_May.csv')\n",
    "\n",
    "# Load Cellulant_Ke_May - Float Acco.csv (Bank Statements) with header=0\n",
    "cellulant_ke_df = pd.read_csv('/Users/gracegitau/Downloads/Cellulant_Ke_May - Float Acco.csv', header=5)\n",
    "\n",
    "# --- 2. Preprocessing for cellulant_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_hex_df.columns = cellulant_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_hex_df = cellulant_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "cellulant_hex_df['Date'] = pd.to_datetime(cellulant_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts (credits/deposits)\n",
    "cellulant_hex_df_recon = cellulant_hex_df[cellulant_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation and create a copy to avoid SettingWithCopyWarning\n",
    "cellulant_hex_df_recon = cellulant_hex_df_recon[['Date', 'Amount', 'Description', 'TRANSFER_ID']].copy()\n",
    "\n",
    "# !!! NEW FIX: Extract only the date component for matching\n",
    "cellulant_hex_df_recon.loc[:, 'Date_Match'] = cellulant_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 3. Preprocessing for cellulant_ke_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_ke_df.columns = cellulant_ke_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_ke_df = cellulant_ke_df.rename(columns={\n",
    "    'DateTime': 'Date',\n",
    "    'Credit Amount': 'Credit',\n",
    "    'Transaction Type': 'Description',\n",
    "    'Customer Float Transaction ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects (handle format like '5/30/25, 3:21 PM GMT+3')\n",
    "cellulant_ke_df['Date'] = pd.to_datetime(cellulant_ke_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "# Remove timezone information to allow merging (if present)\n",
    "cellulant_ke_df['Date'] = cellulant_ke_df['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Extract only the date component for matching\n",
    "cellulant_ke_df.loc[:, 'Date_Match'] = cellulant_ke_df['Date'].dt.date\n",
    "\n",
    "# Convert 'Credit' to numeric, coercing errors to NaN, then fill NaN with 0\n",
    "# Remove '+' and ',' before converting to numeric\n",
    "cellulant_ke_df['Credit'] = cellulant_ke_df['Credit'].astype(str).str.replace('+', '', regex=False).str.replace(',', '', regex=False).astype(float)\n",
    "cellulant_ke_df['Credit'] = cellulant_ke_df['Credit'].fillna(0)\n",
    "\n",
    "# For reconciliation, consider only credit values from the bank statements; ignore debits.\n",
    "cellulant_ke_df['Amount'] = cellulant_ke_df['Credit']\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "cellulant_ke_df_recon = cellulant_ke_df[cellulant_ke_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation and create a copy to avoid SettingWithCopyWarning\n",
    "cellulant_ke_df_recon = cellulant_ke_df_recon[['Date', 'Amount', 'Description', 'ID', 'Date_Match']].copy()\n",
    "\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = cellulant_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = cellulant_ke_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Cellulant): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Cellulant): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits, Cellulant):{discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "# Use .loc to avoid SettingWithCopyWarning\n",
    "cellulant_hex_df_recon.loc[:, 'Amount_Rounded'] = cellulant_hex_df_recon['Amount'].round(2)\n",
    "cellulant_ke_df_recon.loc[:, 'Amount_Rounded'] = cellulant_ke_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date_Match' and 'Amount_Rounded' to find matched and unmatched transactions\n",
    "reconciled_cellulant_df = pd.merge(\n",
    "    cellulant_hex_df_recon.assign(Source_Internal='Internal'), # Add a source column for internal records\n",
    "    cellulant_ke_df_recon.assign(Source_Bank='Bank'),         # Add a source column for bank statements\n",
    "    on=['Date_Match', 'Amount_Rounded'], # Match on date part only\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions\n",
    "matched_cellulant_transactions = reconciled_cellulant_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Identify unmatched internal transactions\n",
    "unmatched_cellulant_internal = reconciled_cellulant_df[reconciled_cellulant_df['Source_Bank'].isna()]\n",
    "\n",
    "# Identify unmatched bank transactions\n",
    "unmatched_cellulant_bank = reconciled_cellulant_df[reconciled_cellulant_df['Source_Internal'].isna()]\n",
    "\n",
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation: {len(cellulant_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation: {len(cellulant_ke_df_recon)}\")\n",
    "print(f\"Matched Credit Transactions: {len(matched_cellulant_transactions)}\")\n",
    "print(f\"Unmatched Internal Credit Records: {len(unmatched_cellulant_internal)}\")\n",
    "print(f\"Unmatched Bank Credit Records: {len(unmatched_cellulant_bank)}\")\n",
    "\n",
    "print(\"\\n--- Unmatched Internal Credit Records ---\")\n",
    "print(unmatched_cellulant_internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09ac27",
   "metadata": {},
   "source": [
    "# ZAMUPAY (PYCS UBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43b79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Zamupay): $866,843,250.00\n",
      "Total Bank Statement Credit Amount (Zamupay, after filters): $866,843,250.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits, Zamupay): $0.00\n",
      "\n",
      "\n",
      "--- Final Summary of Reconciliation (Zamupay) ---\n",
      "Initial Exact Matched Transactions: 17\n",
      "Transactions Matched with 3-day Date Tolerance: 0\n",
      "Transactions Matched by Aggregation: 1\n",
      "Total Matched Transactions: 18\n",
      "Remaining Unmatched Internal Credit Records: 0\n",
      "Remaining Unmatched Bank Credit Records: 0\n",
      "\n",
      "--- Transactions Matched by Aggregation (Zamupay) ---\n",
      "  Date_Match_Internal  Amount_Internal Date_Match_Bank  Amount_Bank\n",
      "0          2025-05-23      100000000.0      2025-05-23  100000000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets with correct headers ---\n",
    "# Load Zamupay_May - Sheet1.csv (Internal Records)\n",
    "zamupay_internal_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/ZamupayHex_May.csv', header=0)\n",
    "\n",
    "# Load Zamupay_May.xlsx - Sheet1.csv (Bank Statements)\n",
    "zamupay_bank_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/Zamupay_May - Sheet1.csv', header=0)\n",
    "\n",
    "# --- 2. Preprocessing for Zamupay Internal Records ---\n",
    "# Clean column names by stripping whitespace\n",
    "zamupay_internal_df.columns = zamupay_internal_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "zamupay_internal_df = zamupay_internal_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects (assuming MM/DD/YYYY format or inferring)\n",
    "zamupay_internal_df['Date'] = pd.to_datetime(zamupay_internal_df['Date'])\n",
    "\n",
    "# Convert 'Amount' to numeric, handling commas and coercing errors\n",
    "zamupay_internal_df['Amount'] = zamupay_internal_df['Amount'].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "# Filter for positive amounts (credits/deposits)\n",
    "zamupay_internal_df_recon = zamupay_internal_df[zamupay_internal_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "zamupay_internal_df_recon.loc[:, 'Date_Match'] = zamupay_internal_df_recon['Date'].dt.date\n",
    "\n",
    "\n",
    "# --- 3. Preprocessing for Zamupay Bank Statements ---\n",
    "# Clean column names by stripping whitespace\n",
    "zamupay_bank_df.columns = zamupay_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "zamupay_bank_df = zamupay_bank_df.rename(columns={\n",
    "    'Tran. Date': 'Date',\n",
    "    'Credit Amt.': 'Amount',\n",
    "    'Particulars': 'Description'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects (assuming DD-MM-YYYY or MM/DD/YYYY format)\n",
    "zamupay_bank_df['Date'] = pd.to_datetime(zamupay_bank_df['Date'])\n",
    "\n",
    "# Convert 'Amount' to numeric, handling commas and coercing errors\n",
    "zamupay_bank_df['Amount'] = zamupay_bank_df['Amount'].astype(str).str.replace(',', '', regex=False).astype(float)\n",
    "\n",
    "# --- Filter out records with \"REVERSAL\" in 'Description' ---\n",
    "# Ensure 'Description' column exists and is string type for filtering\n",
    "if 'Description' in zamupay_bank_df.columns:\n",
    "    zamupay_bank_df = zamupay_bank_df[\n",
    "        ~zamupay_bank_df['Description'].astype(str).str.contains('REVERSAL', case=False, na=False)\n",
    "    ].copy()\n",
    "else:\n",
    "    print(\"Warning: 'Description' (Particulars) column not found in bank statement. Skipping 'REVERSAL' filter.\")\n",
    "\n",
    "\n",
    "# Filter for positive amounts (credits) AFTER reversal filter\n",
    "zamupay_bank_df_recon = zamupay_bank_df[zamupay_bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "zamupay_bank_df_recon.loc[:, 'Date_Match'] = zamupay_bank_df_recon['Date'].dt.date\n",
    "\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = zamupay_internal_df_recon['Amount'].sum()\n",
    "total_bank_credits = zamupay_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Zamupay): ${total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Zamupay, after filters): ${total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits, Zamupay): ${discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level, exact date match) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "zamupay_internal_df_recon.loc[:, 'Amount_Rounded'] = zamupay_internal_df_recon['Amount'].round(2)\n",
    "zamupay_bank_df_recon.loc[:, 'Amount_Rounded'] = zamupay_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date_Match' and 'Amount_Rounded'\n",
    "reconciled_zamupay_df_exact = pd.merge(\n",
    "    zamupay_internal_df_recon.assign(Source_Internal='Internal'),\n",
    "    zamupay_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched, unmatched internal, and unmatched bank transactions\n",
    "matched_zamupay_transactions_exact = reconciled_zamupay_df_exact.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "unmatched_zamupay_internal_after_exact = reconciled_zamupay_df_exact[reconciled_zamupay_df_exact['Source_Bank'].isna()].copy()\n",
    "unmatched_zamupay_bank_after_exact = reconciled_zamupay_df_exact[reconciled_zamupay_df_exact['Source_Internal'].isna()].copy()\n",
    "\n",
    "# --- 6. Reconciliation with Date Tolerance (3 days) ---\n",
    "# Define the columns that should be in the matched_with_tolerance DataFrame\n",
    "matched_tolerance_cols = ['Date_Internal', 'Amount_Internal', 'Date_Match_Internal', 'Source_Internal',\n",
    "                          'Date_Bank', 'Amount_Bank', 'Date_Match_Bank', 'Source_Bank', 'Amount_Rounded']\n",
    "\n",
    "matched_with_tolerance_list = [] # Use a list of dictionaries to build the DataFrame\n",
    "\n",
    "# Convert Date_Match to datetime objects for date arithmetic\n",
    "unmatched_zamupay_internal_after_exact['Date_Match_DT'] = pd.to_datetime(unmatched_zamupay_internal_after_exact['Date_Match'])\n",
    "unmatched_zamupay_bank_after_exact['Date_Match_DT'] = pd.to_datetime(unmatched_zamupay_bank_after_exact['Date_Match'])\n",
    "\n",
    "# Keep track of indices to drop from the original unmatched dataframes\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for i, internal_row in unmatched_zamupay_internal_after_exact.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "\n",
    "    # Define date range for tolerance\n",
    "    start_date = internal_date - pd.Timedelta(days=3)\n",
    "    end_date = internal_date + pd.Timedelta(days=3)\n",
    "\n",
    "    # Find potential matches in unmatched bank records within the date tolerance\n",
    "    potential_matches = unmatched_zamupay_bank_after_exact[\n",
    "        (unmatched_zamupay_bank_after_exact['Amount_Rounded'] == internal_amount) &\n",
    "        (unmatched_zamupay_bank_after_exact['Date_Match_DT'] >= start_date) &\n",
    "        (unmatched_zamupay_bank_after_exact['Date_Match_DT'] <= end_date)\n",
    "    ]\n",
    "\n",
    "    if not potential_matches.empty:\n",
    "        # Take the first match if multiple exist\n",
    "        matched_bank_row = potential_matches.iloc[0]\n",
    "\n",
    "        # Construct a new row for matched_with_tolerance\n",
    "        new_matched_row = {\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Source_Internal': 'Internal',\n",
    "            'Date_Bank': matched_bank_row['Date_Bank'],\n",
    "            'Amount_Bank': matched_bank_row['Amount_Bank'],\n",
    "            'Date_Match_Bank': matched_bank_row['Date_Match'],\n",
    "            'Source_Bank': 'Bank',\n",
    "            'Amount_Rounded': internal_amount # The amount is the same for both\n",
    "        }\n",
    "        matched_with_tolerance_list.append(new_matched_row)\n",
    "\n",
    "        # Mark indices for removal from unmatched lists\n",
    "        matched_internal_indices.append(i)\n",
    "        matched_bank_indices.append(matched_bank_row.name) # Use .name for original index\n",
    "\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "matched_zamupay_with_tolerance = pd.DataFrame(matched_with_tolerance_list, columns=matched_tolerance_cols)\n",
    "\n",
    "\n",
    "# Remove matched records from the unmatched dataframes\n",
    "current_unmatched_internal = unmatched_zamupay_internal_after_exact.drop(matched_internal_indices).drop(columns=['Date_Match_DT'])\n",
    "current_unmatched_bank = unmatched_zamupay_bank_after_exact.drop(matched_bank_indices).drop(columns=['Date_Match_DT'])\n",
    "\n",
    "# --- 7. Reconciliation by Grouping Bank Records (Split Transactions - Refined Logic) ---\n",
    "matched_by_aggregation_list = []\n",
    "# Need a temporary copy of current_unmatched_bank to modify while iterating\n",
    "temp_unmatched_bank_for_agg = current_unmatched_bank.copy()\n",
    "temp_unmatched_bank_for_agg['Date_Match_DT'] = pd.to_datetime(temp_unmatched_bank_for_agg['Date_Match'])\n",
    "\n",
    "# Keep track of original bank indices that form part of a matched aggregation\n",
    "bank_indices_matched_by_agg = []\n",
    "internal_indices_matched_by_agg = []\n",
    "\n",
    "\n",
    "for i, internal_row in current_unmatched_internal.iterrows():\n",
    "    internal_date = pd.to_datetime(internal_row['Date_Match'])\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "\n",
    "    # Define date range for tolerance\n",
    "    start_date = internal_date - pd.Timedelta(days=3)\n",
    "    end_date = internal_date + pd.Timedelta(days=3)\n",
    "\n",
    "    # Get potential bank matches within the date tolerance\n",
    "    potential_bank_records_in_range = temp_unmatched_bank_for_agg[\n",
    "        (temp_unmatched_bank_for_agg['Date_Match_DT'] >= start_date) &\n",
    "        (temp_unmatched_bank_for_agg['Date_Match_DT'] <= end_date)\n",
    "    ]\n",
    "\n",
    "    # Group these potential bank records by date and sum their amounts\n",
    "    grouped_bank_sums = potential_bank_records_in_range.groupby('Date_Match_DT')['Amount_Rounded'].sum().reset_index()\n",
    "\n",
    "    # Find if any aggregated sum matches the internal amount\n",
    "    matched_agg_bank_entry = grouped_bank_sums[\n",
    "        grouped_bank_sums['Amount_Rounded'].round(2) == internal_amount\n",
    "    ]\n",
    "\n",
    "    if not matched_agg_bank_entry.empty:\n",
    "        # Take the first aggregated match\n",
    "        agg_date_dt = matched_agg_bank_entry.iloc[0]['Date_Match_DT']\n",
    "        agg_amount = matched_agg_bank_entry.iloc[0]['Amount_Rounded']\n",
    "\n",
    "        # Get the original individual bank records that sum up to this aggregation\n",
    "        # These are all bank records on the specific agg_date_dt that are still unmatched\n",
    "        contributing_bank_records = temp_unmatched_bank_for_agg[\n",
    "            (temp_unmatched_bank_for_agg['Date_Match_DT'] == agg_date_dt)\n",
    "        ]\n",
    "\n",
    "        # Double check if the sum of these contributing records still equals the internal amount\n",
    "        if contributing_bank_records['Amount_Rounded'].sum().round(2) == internal_amount:\n",
    "            new_matched_row = {\n",
    "                'Date_Internal': internal_row['Date_Internal'],\n",
    "                'Amount_Internal': internal_row['Amount_Internal'],\n",
    "                'Date_Match_Internal': internal_row['Date_Match'],\n",
    "                'Source_Internal': 'Internal',\n",
    "                'Date_Bank': None, # This will be set to the aggregation date\n",
    "                'Amount_Bank': agg_amount,\n",
    "                'Date_Match_Bank': agg_date_dt.date(),\n",
    "                'Source_Bank': 'Bank (Aggregated)',\n",
    "                'Amount_Rounded': internal_amount\n",
    "            }\n",
    "            matched_by_aggregation_list.append(new_matched_row)\n",
    "\n",
    "            # Mark internal index for removal\n",
    "            internal_indices_matched_by_agg.append(i)\n",
    "\n",
    "            # Mark all contributing bank records for removal\n",
    "            bank_indices_matched_by_agg.extend(contributing_bank_records.index.tolist())\n",
    "            # Remove them from temp_unmatched_bank_for_agg to avoid re-matching\n",
    "            temp_unmatched_bank_for_agg = temp_unmatched_bank_for_agg.drop(contributing_bank_records.index)\n",
    "\n",
    "\n",
    "matched_zamupay_by_aggregation = pd.DataFrame(matched_by_aggregation_list, columns=matched_tolerance_cols)\n",
    "\n",
    "# Remove matched records from the current unmatched dataframes\n",
    "final_unmatched_zamupay_internal = current_unmatched_internal.drop(internal_indices_matched_by_agg)\n",
    "# Remove only those bank records that were part of an aggregation\n",
    "final_unmatched_zamupay_bank = current_unmatched_bank.drop(bank_indices_matched_by_agg, errors='ignore')\n",
    "\n",
    "\n",
    "# --- 8. Final Summary of Reconciliation ---\n",
    "total_matched_zamupay = len(matched_zamupay_transactions_exact) + \\\n",
    "                        len(matched_zamupay_with_tolerance) + \\\n",
    "                        len(matched_zamupay_by_aggregation)\n",
    "\n",
    "total_unmatched_zamupay_internal_final = len(final_unmatched_zamupay_internal)\n",
    "total_unmatched_zamupay_bank_final = len(final_unmatched_zamupay_bank)\n",
    "\n",
    "print(\"\\n--- Final Summary of Reconciliation (Zamupay) ---\")\n",
    "print(f\"Initial Exact Matched Transactions: {len(matched_zamupay_transactions_exact)}\")\n",
    "print(f\"Transactions Matched with 3-day Date Tolerance: {len(matched_zamupay_with_tolerance)}\")\n",
    "print(f\"Transactions Matched by Aggregation: {len(matched_zamupay_by_aggregation)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched_zamupay}\")\n",
    "print(f\"Remaining Unmatched Internal Credit Records: {total_unmatched_zamupay_internal_final}\")\n",
    "print(f\"Remaining Unmatched Bank Credit Records: {total_unmatched_zamupay_bank_final}\")\n",
    "\n",
    "if not matched_zamupay_with_tolerance.empty:\n",
    "    print(\"\\n--- Transactions Matched with 3-day Date Tolerance (Zamupay) ---\")\n",
    "    print(matched_zamupay_with_tolerance[['Date_Match_Internal', 'Amount_Internal', 'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not matched_zamupay_by_aggregation.empty:\n",
    "    print(\"\\n--- Transactions Matched by Aggregation (Zamupay) ---\")\n",
    "    print(matched_zamupay_by_aggregation[['Date_Match_Internal', 'Amount_Internal', 'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_zamupay_internal.empty:\n",
    "    print(\"\\n--- Remaining Unmatched Internal Credit Records (Zamupay) ---\")\n",
    "    print(final_unmatched_zamupay_internal[['Date_Match', 'Amount_Internal']])\n",
    "\n",
    "if not final_unmatched_zamupay_bank.empty:\n",
    "    print(\"\\n--- Remaining Unmatched Bank Credit Records (Zamupay) ---\")\n",
    "    print(final_unmatched_zamupay_bank[['Date_Match', 'Amount_Bank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a7628",
   "metadata": {},
   "source": [
    "# SELCOM TZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Amount: 711,000,000.00\n",
      "Total Bank Statement Credit Amount: 0.00\n",
      "Overall Discrepancy: 711,000,000.00\n",
      "\n",
      "Total Internal Credit Records for Reconciliation (Selcom Tanzania): 6\n",
      "Total Bank Statement Credit Records for Reconciliation (Selcom Tanzania): 0\n",
      "Matched Credit Transactions (Selcom Tanzania): 0\n",
      "Unmatched Internal Credit Records (Selcom Tanzania): 6\n",
      "Unmatched Bank Credit Records (Selcom Tanzania): 0\n",
      "\n",
      "--- Unmatched Internal Credit Records (Selcom Tanzania) ---\n",
      "   Date_Match  Amount_Internal\n",
      "0  2025-05-02      150000000.0\n",
      "1  2025-05-05      161000000.0\n",
      "2  2025-05-06      100000000.0\n",
      "3  2025-05-08      100000000.0\n",
      "4  2025-05-22      100000000.0\n",
      "5  2025-05-26      100000000.0\n",
      "\n",
      "--- Unmatched Bank Credit Records (Selcom Tanzania) ---\n",
      "Empty DataFrame\n",
      "Columns: [Date_Match, Amount_Bank]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the datasets ---\n",
    "# Load SelcomHex_May.csv (Internal Records)\n",
    "selcom_hex_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/SelcomHex_May.csv')\n",
    "\n",
    "# Load Selcom_May.xlsx - srirmam.csv (Bank Statements)\n",
    "selcom_bank_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/Selcom_May - srirmam.csv')\n",
    "\n",
    "# --- 2. Preprocessing for selcom_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "selcom_hex_df.columns = selcom_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency: 'TRANSFER_DATE' to 'Date', 'AMOUNT' to 'Amount'\n",
    "selcom_hex_df = selcom_hex_df.rename(columns={'TRANSFER_DATE': 'Date', 'AMOUNT': 'Amount'})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "selcom_hex_df['Date'] = pd.to_datetime(selcom_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts\n",
    "selcom_hex_df_recon = selcom_hex_df[selcom_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "selcom_hex_df_recon.loc[:, 'Date_Match'] = selcom_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 3. Preprocessing for selcom_bank_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "selcom_bank_df.columns = selcom_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency: 'DATE' to 'Date', 'AMOUNT' to 'Amount'\n",
    "selcom_bank_df = selcom_bank_df.rename(columns={'DATE': 'Date', 'AMOUNT': 'Amount'})\n",
    "\n",
    "# Convert 'Date' to datetime objects (assuming day-first format if not standard)\n",
    "selcom_bank_df['Date'] = pd.to_datetime(selcom_bank_df['Date'])\n",
    "\n",
    "# Convert 'Amount' to numeric, coercing errors to NaN, then fill NaN with 0\n",
    "selcom_bank_df['Amount'] = pd.to_numeric(selcom_bank_df['Amount'], errors='coerce').fillna(0)\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "selcom_bank_df_recon = selcom_bank_df[selcom_bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "selcom_bank_df_recon.loc[:, 'Date_Match'] = selcom_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = selcom_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = selcom_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Amount: {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount: {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy: {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "selcom_hex_df_recon.loc[:, 'Amount_Rounded'] = selcom_hex_df_recon['Amount'].round(2)\n",
    "selcom_bank_df_recon.loc[:, 'Amount_Rounded'] = selcom_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date_Match' and 'Amount_Rounded' to find matched and unmatched transactions\n",
    "reconciled_selcom_df = pd.merge(\n",
    "    selcom_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    selcom_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions\n",
    "matched_selcom_transactions = reconciled_selcom_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Identify unmatched internal transactions\n",
    "unmatched_selcom_internal = reconciled_selcom_df[reconciled_selcom_df['Source_Bank'].isna()]\n",
    "\n",
    "# Identify unmatched bank transactions\n",
    "unmatched_selcom_bank = reconciled_selcom_df[reconciled_selcom_df['Source_Internal'].isna()]\n",
    "\n",
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation (Selcom Tanzania): {len(selcom_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation (Selcom Tanzania): {len(selcom_bank_df_recon)}\")\n",
    "print(f\"Matched Credit Transactions (Selcom Tanzania): {len(matched_selcom_transactions)}\")\n",
    "print(f\"Unmatched Internal Credit Records (Selcom Tanzania): {len(unmatched_selcom_internal)}\")\n",
    "print(f\"Unmatched Bank Credit Records (Selcom Tanzania): {len(unmatched_selcom_bank)}\")\n",
    "\n",
    "print(\"\\n--- Unmatched Internal Credit Records (Selcom Tanzania) ---\")\n",
    "print(unmatched_selcom_internal[['Date_Match', 'Amount_Internal']])\n",
    "\n",
    "print(\"\\n--- Unmatched Bank Credit Records (Selcom Tanzania) ---\")\n",
    "print(unmatched_selcom_bank[['Date_Match', 'Amount_Bank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f90f5",
   "metadata": {},
   "source": [
    "# EQUITY TZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1181d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Equity TZ): 462,800,000.00\n",
      "Total Bank Statement Credit Amount (Equity TZ) (Filtered by Narrative): 0.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits, Equity TZ): 462,800,000.00\n",
      "\n",
      "Total Internal Credit Records for Reconciliation (Equity TZ): 5\n",
      "Total Bank Statement Credit Records for Reconciliation (Equity TZ) (Filtered by Narrative): 0\n",
      "Matched Credit Transactions (Equity TZ): 0\n",
      "Unmatched Internal Credit Records (Equity TZ): 5\n",
      "Unmatched Bank Credit Records (Equity TZ): 0\n",
      "\n",
      "--- Unmatched Internal Credit Records (Equity TZ) ---\n",
      "   Date_Match  Amount_Internal\n",
      "0  2025-05-02      100000000.0\n",
      "1  2025-05-12      100000000.0\n",
      "2  2025-05-15      100000000.0\n",
      "3  2025-05-21       89200000.0\n",
      "4  2025-05-28       73600000.0\n",
      "\n",
      "--- Unmatched Bank Credit Records (Equity TZ) ---\n",
      "Empty DataFrame\n",
      "Columns: [Date_Match, Amount_Bank]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets ---\n",
    "# Load EquityTzHex_May.csv (Internal Records)\n",
    "equity_tz_hex_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/EquityTzHex_May.csv')\n",
    "\n",
    "# Load EquityTZ_May - Sheet0.csv (Bank Statements)\n",
    "# Based on previous examples, the header might be in row index 8 (9th row)\n",
    "equity_tz_bank_df = pd.read_csv('/Users/gracegitau/Downloads/Recon May/EquityTZ_May - Sheet0.csv', header=8)\n",
    "\n",
    "# --- 2. Preprocessing for equity_tz_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "equity_tz_hex_df.columns = equity_tz_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency: 'TRANSFER_DATE' to 'Date', 'AMOUNT' to 'Amount'\n",
    "equity_tz_hex_df = equity_tz_hex_df.rename(columns={'TRANSFER_DATE': 'Date', 'AMOUNT': 'Amount'})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "equity_tz_hex_df['Date'] = pd.to_datetime(equity_tz_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts\n",
    "equity_tz_hex_df_recon = equity_tz_hex_df[equity_tz_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "equity_tz_hex_df_recon.loc[:, 'Date_Match'] = equity_tz_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 3. Preprocessing for equity_tz_bank_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "equity_tz_bank_df.columns = equity_tz_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename 'Transaction Date' to 'Date' for consistency\n",
    "equity_tz_bank_df = equity_tz_bank_df.rename(columns={'Transaction Date': 'Date'})\n",
    "\n",
    "# Convert 'Date' to datetime objects (assuming day-first format like '02-05-2025')\n",
    "equity_tz_bank_df['Date'] = pd.to_datetime(equity_tz_bank_df['Date'], dayfirst=True)\n",
    "\n",
    "# Convert 'Credit' to numeric, coercing errors to NaN, then fill NaN with 0\n",
    "equity_tz_bank_df['Credit'] = pd.to_numeric(equity_tz_bank_df['Credit'], errors='coerce').fillna(0)\n",
    "\n",
    "# For reconciliation, consider only credit values from the bank statements; ignore debits.\n",
    "equity_tz_bank_df['Amount'] = equity_tz_bank_df['Credit']\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "equity_tz_bank_df_recon = equity_tz_bank_df[equity_tz_bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# --- Filter bank records by 'Narrative' ---\n",
    "narrative_filter = 'RTGS NALA'\n",
    "# Ensure the narrative is exactly matched, stripping any extra whitespace from the bank statement 'Narrative' column\n",
    "equity_tz_bank_df_recon = equity_tz_bank_df_recon[\n",
    "    equity_tz_bank_df_recon['Narrative'].astype(str).str.strip() == narrative_filter.strip()\n",
    "].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "equity_tz_bank_df_recon.loc[:, 'Date_Match'] = equity_tz_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = equity_tz_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = equity_tz_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Equity TZ): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Equity TZ) (Filtered by Narrative): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits, Equity TZ): {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "equity_tz_hex_df_recon.loc[:, 'Amount_Rounded'] = equity_tz_hex_df_recon['Amount'].round(2)\n",
    "equity_tz_bank_df_recon.loc[:, 'Amount_Rounded'] = equity_tz_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date_Match' and 'Amount_Rounded' to find matched and unmatched transactions\n",
    "reconciled_equity_tz_df = pd.merge(\n",
    "    equity_tz_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    equity_tz_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions\n",
    "matched_equity_tz_transactions = reconciled_equity_tz_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Identify unmatched internal transactions\n",
    "unmatched_equity_tz_internal = reconciled_equity_tz_df[reconciled_equity_tz_df['Source_Bank'].isna()]\n",
    "\n",
    "# Identify unmatched bank transactions\n",
    "unmatched_equity_tz_bank = reconciled_equity_tz_df[reconciled_equity_tz_df['Source_Internal'].isna()]\n",
    "\n",
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation (Equity TZ): {len(equity_tz_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation (Equity TZ) (Filtered by Narrative): {len(equity_tz_bank_df_recon)}\")\n",
    "print(f\"Matched Credit Transactions (Equity TZ): {len(matched_equity_tz_transactions)}\")\n",
    "print(f\"Unmatched Internal Credit Records (Equity TZ): {len(unmatched_equity_tz_internal)}\")\n",
    "print(f\"Unmatched Bank Credit Records (Equity TZ): {len(unmatched_equity_tz_bank)}\")\n",
    "\n",
    "print(\"\\n--- Unmatched Internal Credit Records (Equity TZ) ---\")\n",
    "print(unmatched_equity_tz_internal[['Date_Match', 'Amount_Internal']])\n",
    "\n",
    "print(\"\\n--- Unmatched Bank Credit Records (Equity TZ) ---\")\n",
    "print(unmatched_equity_tz_bank[['Date_Match', 'Amount_Bank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b74da37",
   "metadata": {},
   "source": [
    "# CELLULANT TZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21d5ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Cellulant TZ): 433,225,000.00\n",
      "Total Bank Statement Credit Amount (Cellulant TZ): 433,225,000.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits, Cellulant TZ): 0.00\n",
      "\n",
      "Total Internal Credit Records for Reconciliation (Cellulant TZ): 5\n",
      "Total Bank Statement Credit Records for Reconciliation (Cellulant TZ): 5\n",
      "Matched Credit Transactions (Exact Date & Amount): 0\n",
      "Matched Credit Transactions (Amount & Date within +/- 3 days): 5\n",
      "Total Matched Credit Transactions: 5\n",
      "Truly Unmatched Internal Credit Records: 0\n",
      "Truly Unmatched Bank Credit Records: 0\n",
      "\n",
      "--- Matched Credit Transactions (Amount & Date within +/- 3 days) ---\n",
      "  Date_Match_Internal  Amount_Internal Date_Match_Bank  Amount_Bank\n",
      "0          2025-05-28      100000000.0      2025-05-30  100000000.0\n",
      "1          2025-05-26      100000000.0      2025-05-27  100000000.0\n",
      "2          2025-05-20       32600000.0      2025-05-21   32600000.0\n",
      "3          2025-05-08      100625000.0      2025-05-09  100625000.0\n",
      "4          2025-05-02      100000000.0      2025-05-05  100000000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_46529/73649587.py:50: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  cellulant_tz_bank_df['Date'] = pd.to_datetime(cellulant_tz_bank_df['Date'], infer_datetime_format=True)\n",
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_46529/73649587.py:50: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cellulant_tz_bank_df['Date'] = pd.to_datetime(cellulant_tz_bank_df['Date'], infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets ---\n",
    "# Load CellulantTzHex_May.csv (Internal Records)\n",
    "cellulant_tz_hex_df = pd.read_csv('/Users/gracegitau/Downloads/CellulantTzHex_May.csv')\n",
    "\n",
    "# Load CellulantTz_May - Float Acco.csv (Bank Statements) with header=5 (6th row)\n",
    "cellulant_tz_bank_df = pd.read_csv('/Users/gracegitau/Downloads/CellulantTz_May - Float Acco.csv', header=5)\n",
    "\n",
    "# --- 2. Preprocessing for cellulant_tz_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_tz_hex_df.columns = cellulant_tz_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_tz_hex_df = cellulant_tz_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "cellulant_tz_hex_df['Date'] = pd.to_datetime(cellulant_tz_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts (credits/deposits)\n",
    "cellulant_tz_hex_df_recon = cellulant_tz_hex_df[cellulant_tz_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "cellulant_tz_hex_df_recon = cellulant_tz_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "cellulant_tz_hex_df_recon.loc[:, 'Date_Match'] = cellulant_tz_hex_df_recon['Date'].dt.date\n",
    "cellulant_tz_hex_df_recon.loc[:, 'Amount_Rounded'] = cellulant_tz_hex_df_recon['Amount'].round(2)\n",
    "cellulant_tz_hex_df_recon.loc[:, 'Matched_Internal'] = False\n",
    "\n",
    "\n",
    "# --- 3. Preprocessing for cellulant_tz_bank_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_tz_bank_df.columns = cellulant_tz_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_tz_bank_df = cellulant_tz_bank_df.rename(columns={\n",
    "    'DateTime': 'Date',\n",
    "    'Credit Amount': 'Credit',\n",
    "    'Transaction Type': 'Description',\n",
    "    'Customer Float Transaction ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects (handle format like '5/30/25, 3:21 PM GMT+3')\n",
    "cellulant_tz_bank_df['Date'] = pd.to_datetime(cellulant_tz_bank_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "# Remove timezone information to allow merging (if present)\n",
    "cellulant_tz_bank_df['Date'] = cellulant_tz_bank_df['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Extract only the date component for matching\n",
    "cellulant_tz_bank_df.loc[:, 'Date_Match'] = cellulant_tz_bank_df['Date'].dt.date\n",
    "\n",
    "# Convert 'Credit' to numeric, coercing errors to NaN, then fill NaN with 0\n",
    "# Remove '+' and ',' before converting to numeric\n",
    "cellulant_tz_bank_df['Credit'] = cellulant_tz_bank_df['Credit'].astype(str).str.replace('+', '', regex=False).str.replace(',', '', regex=False).astype(float)\n",
    "cellulant_tz_bank_df['Credit'] = cellulant_tz_bank_df['Credit'].fillna(0)\n",
    "\n",
    "# For reconciliation, consider only credit values from the bank statements; ignore debits.\n",
    "cellulant_tz_bank_df['Amount'] = cellulant_tz_bank_df['Credit']\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "cellulant_tz_bank_df_recon = cellulant_tz_bank_df[cellulant_tz_bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "cellulant_tz_bank_df_recon = cellulant_tz_bank_df_recon[['Date', 'Amount', 'Description', 'ID', 'Date_Match']].copy()\n",
    "cellulant_tz_bank_df_recon.loc[:, 'Amount_Rounded'] = cellulant_tz_bank_df_recon['Amount'].round(2)\n",
    "cellulant_tz_bank_df_recon.loc[:, 'Matched_Bank'] = False\n",
    "\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = cellulant_tz_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = cellulant_tz_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Cellulant TZ): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Cellulant TZ): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits, Cellulant TZ): {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) with Date Tolerance ---\n",
    "\n",
    "# Initialize matched dataframes\n",
    "matched_transactions_exact = pd.DataFrame()\n",
    "matched_transactions_tolerance = pd.DataFrame()\n",
    "\n",
    "# Create copies for iteration\n",
    "temp_internal = cellulant_tz_hex_df_recon.copy()\n",
    "temp_bank = cellulant_tz_bank_df_recon.copy()\n",
    "\n",
    "# Try exact match first (Date_Match and Amount_Rounded)\n",
    "merged_exact = pd.merge(\n",
    "    temp_internal,\n",
    "    temp_bank,\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='inner',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "if not merged_exact.empty:\n",
    "    matched_transactions_exact = pd.concat([matched_transactions_exact, merged_exact])\n",
    "    # Mark matched records in temporary dataframes\n",
    "    temp_internal.loc[temp_internal['ID'].isin(merged_exact['ID_Internal']), 'Matched_Internal'] = True\n",
    "    temp_bank.loc[temp_bank['ID'].isin(merged_exact['ID_Bank']), 'Matched_Bank'] = True\n",
    "\n",
    "# Get remaining unmatched records\n",
    "unmatched_internal_after_exact = temp_internal[~temp_internal['Matched_Internal']].copy()\n",
    "unmatched_bank_after_exact = temp_bank[~temp_bank['Matched_Bank']].copy()\n",
    "\n",
    "# Try matching with date tolerance for remaining unmatched records\n",
    "# Define date tolerance (e.g., +/- 3 days)\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "\n",
    "for index_internal, row_internal in unmatched_internal_after_exact.iterrows():\n",
    "    found_match = False\n",
    "    for index_bank, row_bank in unmatched_bank_after_exact.iterrows():\n",
    "        if (row_internal['Amount_Rounded'] == row_bank['Amount_Rounded']) and \\\n",
    "           (abs(row_internal['Date_Match'] - row_bank['Date_Match']) <= date_tolerance):\n",
    "            # Found a match within tolerance\n",
    "            matched_row = pd.concat([row_internal.rename(lambda x: x + '_Internal'),\n",
    "                                     row_bank.rename(lambda x: x + '_Bank')])\n",
    "            matched_transactions_tolerance = pd.concat([matched_transactions_tolerance, pd.DataFrame([matched_row])], ignore_index=True)\n",
    "            unmatched_internal_after_exact.loc[index_internal, 'Matched_Internal'] = True\n",
    "            unmatched_bank_after_exact.loc[index_bank, 'Matched_Bank'] = True\n",
    "            found_match = True\n",
    "            break # Move to the next internal record once a match is found\n",
    "    # If no exact match was found, and no tolerance match, it remains truly unmatched\n",
    "    # This logic is handled by filtering out 'Matched_' records in the next step\n",
    "\n",
    "# Filter out records that were matched (either exact or by tolerance)\n",
    "final_unmatched_internal = unmatched_internal_after_exact[~unmatched_internal_after_exact['Matched_Internal']].copy()\n",
    "final_unmatched_bank = unmatched_bank_after_exact[~unmatched_bank_after_exact['Matched_Bank']].copy()\n",
    "\n",
    "\n",
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation (Cellulant TZ): {len(cellulant_tz_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation (Cellulant TZ): {len(cellulant_tz_bank_df_recon)}\")\n",
    "print(f\"Matched Credit Transactions (Exact Date & Amount): {len(matched_transactions_exact)}\")\n",
    "print(f\"Matched Credit Transactions (Amount & Date within +/- {date_tolerance.days} days): {len(matched_transactions_tolerance)}\")\n",
    "print(f\"Total Matched Credit Transactions: {len(matched_transactions_exact) + len(matched_transactions_tolerance)}\")\n",
    "print(f\"Truly Unmatched Internal Credit Records: {len(final_unmatched_internal)}\")\n",
    "print(f\"Truly Unmatched Bank Credit Records: {len(final_unmatched_bank)}\")\n",
    "\n",
    "if not matched_transactions_tolerance.empty:\n",
    "    print(f\"\\n--- Matched Credit Transactions (Amount & Date within +/- {date_tolerance.days} days) ---\")\n",
    "    print(matched_transactions_tolerance[['Date_Match_Internal', 'Amount_Internal', 'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\n--- Truly Unmatched Internal Credit Records (Cellulant TZ) ---\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount']])\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\n--- Truly Unmatched Bank Credit Records (Cellulant TZ) ---\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645c6fd",
   "metadata": {},
   "source": [
    "# FLUTTERWAVE UGANDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d800724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Amount: 1,663,181,500.00\n",
      "Total Bank Credit Amount: 1,663,181,500.00\n",
      "Overall Discrepancy: 0.00\n",
      "\n",
      "\n",
      "--- Summary of Reconciliation (with 3-day date tolerance) ---\n",
      "Initial Exact Matched Transactions: 2\n",
      "Transactions Matched with 3-day Date Tolerance: 5\n",
      "Total Matched Transactions: 7\n",
      "Remaining Unmatched Internal Credit Records: 0\n",
      "Remaining Unmatched Bank Credit Records: 0\n",
      "\n",
      "--- Transactions Matched with 3-day Date Tolerance ---\n",
      "  Date_Match_Internal  Amount_Internal Date_Match_Bank  Amount_Bank\n",
      "0          2025-05-06      201800000.0      2025-05-07  201800000.0\n",
      "1          2025-05-19      176610000.0      2025-05-20  176610000.0\n",
      "2          2025-05-21      278214000.0      2025-05-22  278214000.0\n",
      "3          2025-05-22      169595000.0      2025-05-23  169595000.0\n",
      "4          2025-05-29      106687500.0      2025-05-30  106687500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_46529/2921377462.py:44: UserWarning: Parsing dates in %Y-%m-%dT%H:%M:%S%z format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  bank_df['Date'] = pd.to_datetime(bank_df['Date'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the datasets ---\n",
    "# Load FW.Ug.Hex_May.csv (Internal Records)\n",
    "internal_df = pd.read_csv('/Users/gracegitau/Downloads/FW.Ug.Hex_May.csv')\n",
    "\n",
    "# Load FW.Ug_May.csv (Bank Statements)\n",
    "bank_df = pd.read_csv('/Users/gracegitau/Downloads/FW.Ug_May.csv')\n",
    "\n",
    "# --- 2. Preprocessing for internal_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "internal_df.columns = internal_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency: 'TRANSFER_DATE' to 'Date', 'AMOUNT' to 'Amount'\n",
    "internal_df = internal_df.rename(columns={'TRANSFER_DATE': 'Date', 'AMOUNT': 'Amount'})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "internal_df['Date'] = pd.to_datetime(internal_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts\n",
    "internal_df_recon = internal_df[internal_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "internal_df_recon.loc[:, 'Date_Match'] = internal_df_recon['Date'].dt.date\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "internal_df_recon = internal_df_recon[['Date', 'Amount', 'Date_Match']]\n",
    "\n",
    "# --- 3. Preprocessing for bank_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "bank_df.columns = bank_df.columns.str.strip()\n",
    "\n",
    "# Function to find column based on keywords (case-insensitive)\n",
    "def find_column(df, keywords):\n",
    "    for col in df.columns:\n",
    "        if any(keyword.lower() in col.lower() for keyword in keywords):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# Dynamically find 'Date' column\n",
    "date_col_bank = find_column(bank_df, ['date', 'value date', 'transaction date'])\n",
    "if date_col_bank:\n",
    "    bank_df = bank_df.rename(columns={date_col_bank: 'Date'})\n",
    "    bank_df['Date'] = pd.to_datetime(bank_df['Date'], dayfirst=True)\n",
    "else:\n",
    "    raise KeyError(\"Error: 'Date' column not found in bank statement. Please ensure the bank statement contains a date column (e.g., 'Date', 'Value Date', 'Transaction Date').\")\n",
    "\n",
    "# Dynamically find 'Amount' column\n",
    "amount_col_bank = find_column(bank_df, ['amount', 'credit'])\n",
    "if amount_col_bank:\n",
    "    bank_df = bank_df.rename(columns={amount_col_bank: 'Amount'})\n",
    "    bank_df['Amount'] = pd.to_numeric(bank_df['Amount'], errors='coerce').fillna(0)\n",
    "else:\n",
    "    raise KeyError(\"Error: 'Amount' (or 'Credit') column not found in bank statement. Please ensure the bank statement contains an amount/credit column.\")\n",
    "\n",
    "# Dynamically find 'type' column\n",
    "type_col_bank = find_column(bank_df, ['type'])\n",
    "if type_col_bank:\n",
    "    bank_df = bank_df.rename(columns={type_col_bank: 'Type'})\n",
    "else:\n",
    "    print(\"Warning: 'Type' column not found in bank statement. Proceeding without 'type' filtering.\")\n",
    "\n",
    "# Dynamically find 'remarks' column\n",
    "remarks_col_bank = find_column(bank_df, ['remarks', 'narration'])\n",
    "if remarks_col_bank:\n",
    "    bank_df = bank_df.rename(columns={remarks_col_bank: 'Remarks'})\n",
    "else:\n",
    "    print(\"Warning: 'Remarks' column not found in bank statement. Proceeding without 'rvsl' filtering.\")\n",
    "\n",
    "# Apply the new filters\n",
    "# Filter for 'Type' = 'C' (Credits) if 'Type' column exists\n",
    "if 'Type' in bank_df.columns:\n",
    "    bank_df = bank_df[bank_df['Type'].astype(str).str.upper() == 'C'].copy()\n",
    "\n",
    "# Filter out records with 'rvsl' in 'Remarks' if 'Remarks' column exists\n",
    "if 'Remarks' in bank_df.columns:\n",
    "    bank_df = bank_df[~bank_df['Remarks'].astype(str).str.contains('rvsl', case=False, na=False)].copy()\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits) after all other filters\n",
    "bank_df_recon = bank_df[bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "bank_df_recon.loc[:, 'Date_Match'] = bank_df_recon['Date'].dt.date\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "bank_df_recon = bank_df_recon[['Date', 'Amount', 'Date_Match']]\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = internal_df_recon['Amount'].sum()\n",
    "total_bank_credits = bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Amount: {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Credit Amount: {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy: {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "# --- 5. Initial Reconciliation (transaction-level, exact date match) ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "internal_df_recon.loc[:, 'Amount_Rounded'] = internal_df_recon['Amount'].round(2)\n",
    "bank_df_recon.loc[:, 'Amount_Rounded'] = bank_df_recon['Amount'].round(2)\n",
    "\n",
    "reconciled_df_exact = pd.merge(\n",
    "    internal_df_recon.assign(Source_Internal='Internal'),\n",
    "    bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "matched_exact = reconciled_df_exact.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "unmatched_internal_after_exact = reconciled_df_exact[reconciled_df_exact['Source_Bank'].isna()].copy()\n",
    "unmatched_bank_after_exact = reconciled_df_exact[reconciled_df_exact['Source_Internal'].isna()].copy()\n",
    "\n",
    "# --- 6. Reconciliation with Date Tolerance (3 days) ---\n",
    "# Define the columns that should be in the matched_with_tolerance DataFrame\n",
    "# These columns are derived from the structure of reconciled_df_exact\n",
    "matched_tolerance_cols = ['Date_Internal', 'Amount_Internal', 'Date_Match_Internal', 'Source_Internal',\n",
    "                          'Date_Bank', 'Amount_Bank', 'Date_Match_Bank', 'Source_Bank', 'Amount_Rounded']\n",
    "\n",
    "matched_with_tolerance_list = [] # Use a list of dictionaries to build the DataFrame\n",
    "\n",
    "# Convert Date_Match to datetime objects for date arithmetic\n",
    "unmatched_internal_after_exact['Date_Match_DT'] = pd.to_datetime(unmatched_internal_after_exact['Date_Match'])\n",
    "unmatched_bank_after_exact['Date_Match_DT'] = pd.to_datetime(unmatched_bank_after_exact['Date_Match'])\n",
    "\n",
    "# Keep track of indices to drop from the original unmatched dataframes\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for i, internal_row in unmatched_internal_after_exact.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "\n",
    "    # Define date range for tolerance\n",
    "    start_date = internal_date - pd.Timedelta(days=3)\n",
    "    end_date = internal_date + pd.Timedelta(days=3)\n",
    "\n",
    "    # Find potential matches in unmatched bank records within the date tolerance\n",
    "    potential_matches = unmatched_bank_after_exact[\n",
    "        (unmatched_bank_after_exact['Amount_Rounded'] == internal_amount) &\n",
    "        (unmatched_bank_after_exact['Date_Match_DT'] >= start_date) &\n",
    "        (unmatched_bank_after_exact['Date_Match_DT'] <= end_date)\n",
    "    ]\n",
    "\n",
    "    if not potential_matches.empty:\n",
    "        # Take the first match if multiple exist\n",
    "        matched_bank_row = potential_matches.iloc[0]\n",
    "\n",
    "        # Construct a new row for matched_with_tolerance\n",
    "        new_matched_row = {\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Source_Internal': 'Internal',\n",
    "            'Date_Bank': matched_bank_row['Date_Bank'],\n",
    "            'Amount_Bank': matched_bank_row['Amount_Bank'],\n",
    "            'Date_Match_Bank': matched_bank_row['Date_Match'],\n",
    "            'Source_Bank': 'Bank',\n",
    "            'Amount_Rounded': internal_amount # The amount is the same for both\n",
    "        }\n",
    "        matched_with_tolerance_list.append(new_matched_row)\n",
    "\n",
    "        # Mark indices for removal from unmatched lists\n",
    "        matched_internal_indices.append(i)\n",
    "        matched_bank_indices.append(matched_bank_row.name) # Use .name for original index\n",
    "\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "matched_with_tolerance = pd.DataFrame(matched_with_tolerance_list, columns=matched_tolerance_cols)\n",
    "\n",
    "\n",
    "# Remove matched records from the unmatched dataframes\n",
    "final_unmatched_internal = unmatched_internal_after_exact.drop(matched_internal_indices).drop(columns=['Date_Match_DT'])\n",
    "final_unmatched_bank = unmatched_bank_after_exact.drop(matched_bank_indices).drop(columns=['Date_Match_DT'])\n",
    "\n",
    "# --- 7. Final Summary of Reconciliation ---\n",
    "total_matched = len(matched_exact) + len(matched_with_tolerance)\n",
    "total_unmatched_internal_final = len(final_unmatched_internal)\n",
    "total_unmatched_bank_final = len(final_unmatched_bank)\n",
    "\n",
    "print(\"\\n--- Summary of Reconciliation (with 3-day date tolerance) ---\")\n",
    "print(f\"Initial Exact Matched Transactions: {len(matched_exact)}\")\n",
    "print(f\"Transactions Matched with 3-day Date Tolerance: {len(matched_with_tolerance)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched}\")\n",
    "print(f\"Remaining Unmatched Internal Credit Records: {total_unmatched_internal_final}\")\n",
    "print(f\"Remaining Unmatched Bank Credit Records: {total_unmatched_bank_final}\")\n",
    "\n",
    "if not matched_with_tolerance.empty:\n",
    "    print(\"\\n--- Transactions Matched with 3-day Date Tolerance ---\")\n",
    "    # Display relevant columns, showing both dates to highlight the tolerance\n",
    "    print(matched_with_tolerance[['Date_Match_Internal', 'Amount_Internal', 'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\n--- Remaining Unmatched Internal Credit Records ---\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount_Internal']])\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\n--- Remaining Unmatched Bank Credit Records ---\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount_Bank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591b35a",
   "metadata": {},
   "source": [
    "# NIGERIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ea546",
   "metadata": {},
   "source": [
    "## Cellulant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fde0fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Cellulant NG): 2,235,500,000.00\n",
      "Total Bank Statement Credit Amount (Cellulant NG): 2,917,398,000.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits, Cellulant NG): -681,898,000.00\n",
      "\n",
      "Total Internal Credit Records for Reconciliation (Cellulant NG): 6\n",
      "Total Bank Statement Credit Records for Reconciliation (Cellulant NG): 7\n",
      "Exact Matched Credit Transactions: 5\n",
      "Transactions Matched with Date Tolerance: 0\n",
      "Total Matched Transactions: 5\n",
      "Unmatched Internal Credit Records: 1\n",
      "Unmatched Bank Credit Records: 2\n",
      "\n",
      "\n",
      "--- Unmatched Internal Credit Records ---\n",
      "   Date_Match  Amount_Internal\n",
      "2  2025-06-13      481900000.0\n",
      "\n",
      "--- Unmatched Bank Credit Records ---\n",
      "   Date_Match  Amount_Bank\n",
      "1  2025-06-13  481899000.0\n",
      "3  2025-06-13  681899000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_37297/528926428.py:60: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  cellulant_ng_bank_df['Date'] = pd.to_datetime(cellulant_ng_bank_df['Date'], infer_datetime_format=True)\n",
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_37297/528926428.py:60: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cellulant_ng_bank_df['Date'] = pd.to_datetime(cellulant_ng_bank_df['Date'], infer_datetime_format=True)\n",
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_37297/528926428.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unmatched_cellulant_ng_internal['Date_Match_DT'] = pd.to_datetime(unmatched_cellulant_ng_internal['Date_Match'])\n",
      "/var/folders/p2/4fn8fy8n1sj1qf8ygrmybxjc0000n0/T/ipykernel_37297/528926428.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unmatched_cellulant_ng_bank['Date_Match_DT'] = pd.to_datetime(unmatched_cellulant_ng_bank['Date_Match'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "       \n",
    "# --- 1. Load the datasets ---\n",
    "# Load Cellulant Nigeria Internal Records (CSV)\n",
    "cellulant_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/CNL Hex.csv')\n",
    "\n",
    "# Load Cellulant Nigeria Bank Statements (Excel)\n",
    "try:\n",
    "    cellulant_ng_bank_df = pd.read_excel(\n",
    "        '/Users/gracegitau/Downloads/#Recon May/NGN/CNL Bank.xlsx', \n",
    "        engine='openpyxl', header=5\n",
    "    )\n",
    "except ImportError:\n",
    "    try:\n",
    "        cellulant_ng_bank_df = pd.read_excel(\n",
    "            '/Users/gracegitau/Downloads/#Recon May/NGN/CNL Bank.xlsx', \n",
    "            engine='xlrd', header=5\n",
    "        )\n",
    "    except ImportError:\n",
    "        print(\"Error: Please install either openpyxl or xlrd package to read Excel files\")\n",
    "        print(\"Install with: pip install openpyxl xlrd\")\n",
    "        \n",
    "# --- 2. Preprocessing for cellulant_ng_hex_df (Internal Records) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_ng_hex_df.columns = cellulant_ng_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_ng_hex_df = cellulant_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "cellulant_ng_hex_df['Date'] = pd.to_datetime(cellulant_ng_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts (credits/deposits)\n",
    "cellulant_ng_hex_df_recon = cellulant_ng_hex_df[cellulant_ng_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "cellulant_ng_hex_df_recon = cellulant_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "cellulant_ng_hex_df_recon.loc[:, 'Date_Match'] = cellulant_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# --- 3. Preprocessing for cellulant_ng_bank_df (Bank Statements) ---\n",
    "# Clean column names by stripping whitespace\n",
    "cellulant_ng_bank_df.columns = cellulant_ng_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "cellulant_ng_bank_df = cellulant_ng_bank_df.rename(columns={\n",
    "    'DateTime': 'Date',\n",
    "    'Credit Amount': 'Credit',\n",
    "    'Transaction Type': 'Transaction_Type',\n",
    "    'Customer Float Transaction ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects (handle format like '5/30/25, 3:21 PM GMT+3')\n",
    "cellulant_ng_bank_df['Date'] = pd.to_datetime(cellulant_ng_bank_df['Date'], infer_datetime_format=True)\n",
    "\n",
    "# Remove timezone information to allow merging (if present)\n",
    "cellulant_ng_bank_df['Date'] = cellulant_ng_bank_df['Date'].dt.tz_localize(None)\n",
    "\n",
    "# Extract only the date component for matching\n",
    "cellulant_ng_bank_df.loc[:, 'Date_Match'] = cellulant_ng_bank_df['Date'].dt.date\n",
    "\n",
    "# --- Nigeria Specific Filters ---\n",
    "# Filter for Transaction Type = 'allocate' or 'revoke'\n",
    "cellulant_ng_bank_df = cellulant_ng_bank_df[\n",
    "    cellulant_ng_bank_df['Transaction_Type'].isin(['allocate', 'revoke'])\n",
    "].copy()\n",
    "\n",
    "# Filter for Transaction ID = 1 (if this is a requirement)\n",
    "# Note: Assuming 'Transaction ID' is a column in the Nigeria data\n",
    "if 'Transaction ID' in cellulant_ng_bank_df.columns:\n",
    "    cellulant_ng_bank_df = cellulant_ng_bank_df[\n",
    "        cellulant_ng_bank_df['Transaction ID'] == 1\n",
    "    ].copy()\n",
    "\n",
    "# Process Credit Amount - remove '+' and ',' then convert to numeric\n",
    "cellulant_ng_bank_df['Credit'] = cellulant_ng_bank_df['Credit'].astype(str).str.replace('+', '', regex=False).str.replace(',', '', regex=False).astype(float)\n",
    "cellulant_ng_bank_df['Credit'] = cellulant_ng_bank_df['Credit'].fillna(0)\n",
    "\n",
    "# Process Debit Amount - remove '+' and ',' then convert to numeric\n",
    "if 'Debit Amount' in cellulant_ng_bank_df.columns:\n",
    "    cellulant_ng_bank_df['Debit Amount'] = cellulant_ng_bank_df['Debit Amount'].astype(str).str.replace('+', '', regex=False).str.replace(',', '', regex=False).astype(float)\n",
    "    cellulant_ng_bank_df['Debit Amount'] = cellulant_ng_bank_df['Debit Amount'].fillna(0)\n",
    "\n",
    "# For reconciliation, consider only credit values from the bank statements; ignore debits.\n",
    "cellulant_ng_bank_df['Amount'] = cellulant_ng_bank_df['Credit']\n",
    "\n",
    "# Filter bank records to include only positive amounts (credits)\n",
    "cellulant_ng_bank_df_recon = cellulant_ng_bank_df[cellulant_ng_bank_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "cellulant_ng_bank_df_recon = cellulant_ng_bank_df_recon[['Date', 'Amount', 'Transaction_Type', 'ID', 'Date_Match']].copy()\n",
    "\n",
    "# --- 4. Calculate Total Amounts and Discrepancy (before reconciliation) ---\n",
    "total_internal_credits = cellulant_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = cellulant_ng_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Cellulant NG): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Cellulant NG): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits, Cellulant NG): {discrepancy_amount:,.2f}\\n\")\n",
    "\n",
    "# --- 5. Reconciliation (transaction-level) with Date Tolerance ---\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "cellulant_ng_hex_df_recon.loc[:, 'Amount_Rounded'] = cellulant_ng_hex_df_recon['Amount'].round(2)\n",
    "cellulant_ng_bank_df_recon.loc[:, 'Amount_Rounded'] = cellulant_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge on 'Date_Match' and 'Amount_Rounded' to find matched and unmatched transactions\n",
    "reconciled_cellulant_ng_df = pd.merge(\n",
    "    cellulant_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    cellulant_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions (present in both internal and bank)\n",
    "matched_cellulant_ng_transactions = reconciled_cellulant_ng_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Identify unmatched internal transactions (present in internal but not in bank)\n",
    "unmatched_cellulant_ng_internal = reconciled_cellulant_ng_df[reconciled_cellulant_ng_df['Source_Bank'].isna()]\n",
    "\n",
    "# Identify unmatched bank transactions (present in bank but not in internal)\n",
    "unmatched_cellulant_ng_bank = reconciled_cellulant_ng_df[reconciled_cellulant_ng_df['Source_Internal'].isna()]\n",
    "\n",
    "# --- Additional matching with date tolerance (3 days) ---\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "matched_with_tolerance = []\n",
    "\n",
    "# Convert Date_Match to datetime for comparison\n",
    "unmatched_cellulant_ng_internal['Date_Match_DT'] = pd.to_datetime(unmatched_cellulant_ng_internal['Date_Match'])\n",
    "unmatched_cellulant_ng_bank['Date_Match_DT'] = pd.to_datetime(unmatched_cellulant_ng_bank['Date_Match'])\n",
    "\n",
    "for idx, internal_row in unmatched_cellulant_ng_internal.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "    \n",
    "    # Find potential matches in bank records within date tolerance\n",
    "    potential_matches = unmatched_cellulant_ng_bank[\n",
    "        (unmatched_cellulant_ng_bank['Amount_Rounded'] == internal_amount) & \n",
    "        (abs(unmatched_cellulant_ng_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "    ]\n",
    "    \n",
    "    if not potential_matches.empty:\n",
    "        # Take the first match\n",
    "        bank_match = potential_matches.iloc[0]\n",
    "        matched_with_tolerance.append({\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Date_Bank': bank_match['Date_Bank'],\n",
    "            'Amount_Bank': bank_match['Amount_Bank'],\n",
    "            'Date_Match_Bank': bank_match['Date_Match'],\n",
    "            'Amount_Rounded': internal_amount\n",
    "        })\n",
    "        \n",
    "        # Remove matched records from unmatched dataframes\n",
    "        unmatched_cellulant_ng_internal = unmatched_cellulant_ng_internal.drop(idx)\n",
    "        unmatched_cellulant_ng_bank = unmatched_cellulant_ng_bank.drop(bank_match.name)\n",
    "\n",
    "# Convert matched_with_tolerance to DataFrame\n",
    "matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "\n",
    "# --- 6. Summary of Reconciliation ---\n",
    "print(f\"Total Internal Credit Records for Reconciliation (Cellulant NG): {len(cellulant_ng_hex_df_recon)}\")\n",
    "print(f\"Total Bank Statement Credit Records for Reconciliation (Cellulant NG): {len(cellulant_ng_bank_df_recon)}\")\n",
    "print(f\"Exact Matched Credit Transactions: {len(matched_cellulant_ng_transactions)}\")\n",
    "print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "print(f\"Total Matched Transactions: {len(matched_cellulant_ng_transactions) + len(matched_with_tolerance_df)}\")\n",
    "print(f\"Unmatched Internal Credit Records: {len(unmatched_cellulant_ng_internal)}\")\n",
    "print(f\"Unmatched Bank Credit Records: {len(unmatched_cellulant_ng_bank)}\\n\")\n",
    "\n",
    "if not matched_with_tolerance_df.empty:\n",
    "    print(\"\\n--- Transactions Matched with Date Tolerance ---\")\n",
    "    print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', 'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not unmatched_cellulant_ng_internal.empty:\n",
    "    print(\"\\n--- Unmatched Internal Credit Records ---\")\n",
    "    print(unmatched_cellulant_ng_internal[['Date_Match', 'Amount_Internal']])\n",
    "\n",
    "if not unmatched_cellulant_ng_bank.empty:\n",
    "    print(\"\\n--- Unmatched Bank Credit Records ---\")\n",
    "    print(unmatched_cellulant_ng_bank[['Date_Match', 'Amount_Bank']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bc496",
   "metadata": {},
   "source": [
    "## Verto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2ce5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Verto NG): 17,143,290,500.00\n",
      "Total Bank Statement Credit Amount (Verto NG): 17,143,490,500.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits): -200,000.00\n",
      "\n",
      "Step 5: Performing transaction-level reconciliation...\n",
      "\n",
      "Additional Matches Found After Aggregating Same-Day Internal Transactions:\n",
      "   Date_Match  Aggregated_Internal_Amount   Bank_Amount   Bank_Date\n",
      "0  2025-06-12                7.850000e+08  7.850000e+08  2025-06-12\n",
      "1  2025-06-13                1.481900e+09  1.481900e+09  2025-06-13\n",
      "2  2025-06-20                1.488200e+09  1.488200e+09  2025-06-20\n",
      "3  2025-06-27                1.495200e+09  1.495200e+09  2025-06-27\n",
      "4  2025-06-30                8.552000e+08  8.552000e+08  2025-06-30\n",
      "\n",
      "Reconciliation Summary (Verto Nigeria):\n",
      "Total Internal Credit Records: 30\n",
      "Total Bank Credit Records: 25\n",
      "Exact Matched Transactions: 19\n",
      "Transactions Matched with Date Tolerance: 0\n",
      "Total Matched Transactions: 19\n",
      "Remaining Unmatched Internal Records: 1\n",
      "Remaining Unmatched Bank Records: 1\n",
      "\n",
      "Unmatched Internal Records:\n",
      "   Date_Match  Amount_Internal\n",
      "3  2025-06-04      428400000.0\n",
      "\n",
      "Unmatched Bank Records:\n",
      "   Date_Match  Amount_Bank\n",
      "4  2025-06-04  428600000.0\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the datasets ---\n",
    "# Load Verto Nigeria Internal Records (CSV)\n",
    "verto_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/Verto Hex.csv')\n",
    "verto_ng_bank_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/Verto Bank.csv', header = 8)\n",
    "\n",
    "# 2. Preprocessing for internal_df (Internal Records)\n",
    "verto_ng_hex_df.columns = verto_ng_hex_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "verto_ng_hex_df = verto_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime objects\n",
    "verto_ng_hex_df['Date'] = pd.to_datetime(verto_ng_hex_df['Date'])\n",
    "\n",
    "# Filter internal records to include only positive amounts (credits/deposits)\n",
    "verto_ng_hex_df_recon = verto_ng_hex_df[verto_ng_hex_df['Amount'] > 0].copy()\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "verto_ng_hex_df_recon = verto_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "verto_ng_hex_df_recon.loc[:, 'Date_Match'] = verto_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# 3. Preprocessing for bank_df (Bank Statements)\n",
    "# Clean column names by stripping whitespace\n",
    "verto_ng_bank_df.columns = verto_ng_bank_df.columns.str.strip()\n",
    "\n",
    "# Rename columns for consistency\n",
    "verto_ng_bank_df = verto_ng_bank_df.rename(columns={\n",
    "    'Date': 'Date',\n",
    "    'Credit': 'Credit',\n",
    "    'Verto Transaction Id': 'Transaction_ID',\n",
    "    'Comment': 'Description'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime - Nigerian format (day/month/year)\n",
    "verto_ng_bank_df['Date'] = pd.to_datetime(verto_ng_bank_df['Date'], dayfirst=True)\n",
    "\n",
    "# Verto Specific Filters\n",
    "# 1. Filter for June transactions (month = 6, year = 2025)\n",
    "verto_ng_bank_df = verto_ng_bank_df[\n",
    "    (verto_ng_bank_df['Date'].dt.month == 6) & \n",
    "    (verto_ng_bank_df['Date'].dt.year == 2025)\n",
    "].copy()\n",
    "\n",
    "# 2. Process Credit Amount - remove commas and convert to numeric\n",
    "verto_ng_bank_df['Credit'] = (\n",
    "    verto_ng_bank_df['Credit'].astype(str)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .replace('', '0')  # Handle empty strings\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# 3. Filter for positive credits only\n",
    "verto_ng_bank_df = verto_ng_bank_df[\n",
    "    verto_ng_bank_df['Credit'] > 0\n",
    "].copy()\n",
    "\n",
    "# For reconciliation, use the credit values\n",
    "verto_ng_bank_df['Amount'] = verto_ng_bank_df['Credit']\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "verto_ng_bank_df_recon = verto_ng_bank_df[['Date', 'Amount', 'Description', 'Transaction_ID']].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "verto_ng_bank_df_recon.loc[:, 'Date_Match'] = verto_ng_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# 4. Calculate Total Amounts and Discrepancy (before reconciliation)\n",
    "total_internal_credits = verto_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = verto_ng_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Verto NG): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Verto NG): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits): {discrepancy_amount:,.2f}\")\n",
    "\n",
    "# 5. Reconciliation (transaction-level) with Date Tolerance\n",
    "print(\"\\nStep 5: Performing transaction-level reconciliation...\")\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "verto_ng_hex_df_recon.loc[:, 'Amount_Rounded'] = verto_ng_hex_df_recon['Amount'].round(2)\n",
    "verto_ng_bank_df_recon.loc[:, 'Amount_Rounded'] = verto_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "# Perform an outer merge to find matched and unmatched transactions\n",
    "reconciled_df = pd.merge(\n",
    "    verto_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    verto_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "# Identify matched transactions\n",
    "matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "# Additional matching with 3-day date tolerance\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "\n",
    "# Convert Date_Match to datetime for comparison\n",
    "unmatched_internal['Date_Match_DT'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "unmatched_bank['Date_Match_DT'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "\n",
    "matched_with_tolerance = []\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for idx, internal_row in unmatched_internal.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "    \n",
    "    potential_matches = unmatched_bank[\n",
    "        (unmatched_bank['Amount_Rounded'] == internal_amount) &\n",
    "        (abs(unmatched_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "    ]\n",
    "    \n",
    "    if not potential_matches.empty:\n",
    "        bank_match = potential_matches.iloc[0]\n",
    "        matched_with_tolerance.append({\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Date_Bank': bank_match['Date_Bank'],\n",
    "            'Amount_Bank': bank_match['Amount_Bank'],\n",
    "            'Date_Match_Bank': bank_match['Date_Match'],\n",
    "            'Amount_Rounded': internal_amount\n",
    "        })\n",
    "        matched_internal_indices.append(idx)\n",
    "        matched_bank_indices.append(bank_match.name)\n",
    "\n",
    "# Create DataFrame from tolerance matches\n",
    "matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "\n",
    "# Remove matched records from unmatched dataframes\n",
    "final_unmatched_internal = unmatched_internal.drop(matched_internal_indices)\n",
    "final_unmatched_bank = unmatched_bank.drop(matched_bank_indices)\n",
    "\n",
    "# After the initial reconciliation (Step 5), add this additional matching logic:\n",
    "\n",
    "# 5a. Merge same-day internal transactions before matching with bank\n",
    "if len(final_unmatched_internal) > 0:\n",
    "    # Group internal transactions by date and sum amounts\n",
    "    internal_aggregated = (\n",
    "        final_unmatched_internal.groupby('Date_Match')\n",
    "        .agg({'Amount_Internal': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    internal_aggregated['Amount_Rounded'] = internal_aggregated['Amount_Internal'].round(2)\n",
    "    internal_aggregated['Date_Match_DT'] = pd.to_datetime(internal_aggregated['Date_Match'])\n",
    "    \n",
    "    # Try matching aggregated internal transactions with bank\n",
    "    bank_unmatched = final_unmatched_bank.copy()\n",
    "    bank_unmatched['Date_Match_DT'] = pd.to_datetime(bank_unmatched['Date_Match'])\n",
    "    \n",
    "    matched_aggregated = []\n",
    "    matched_bank_indices = []\n",
    "    \n",
    "    for idx, internal_row in internal_aggregated.iterrows():\n",
    "        internal_date = internal_row['Date_Match_DT']\n",
    "        internal_amount = internal_row['Amount_Rounded']\n",
    "        \n",
    "        potential_matches = bank_unmatched[\n",
    "            (bank_unmatched['Amount_Rounded'] == internal_amount) &\n",
    "            (bank_unmatched['Date_Match_DT'] == internal_date)\n",
    "        ]\n",
    "        \n",
    "        if not potential_matches.empty:\n",
    "            bank_match = potential_matches.iloc[0]\n",
    "            matched_aggregated.append({\n",
    "                'Date_Match': internal_row['Date_Match'],\n",
    "                'Aggregated_Internal_Amount': internal_row['Amount_Internal'],\n",
    "                'Bank_Amount': bank_match['Amount_Bank'],\n",
    "                'Bank_Date': bank_match['Date_Match']\n",
    "            })\n",
    "            matched_bank_indices.append(bank_match.name)\n",
    "    \n",
    "    # Update unmatched records\n",
    "    if matched_aggregated:\n",
    "        matched_aggregated_df = pd.DataFrame(matched_aggregated)\n",
    "        final_unmatched_bank = final_unmatched_bank.drop(matched_bank_indices)\n",
    "        \n",
    "        # Remove the matched internal transactions\n",
    "        matched_dates = matched_aggregated_df['Date_Match'].unique()\n",
    "        final_unmatched_internal = final_unmatched_internal[\n",
    "            ~final_unmatched_internal['Date_Match'].isin(matched_dates)\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAdditional Matches Found After Aggregating Same-Day Internal Transactions:\")\n",
    "        print(matched_aggregated_df)\n",
    "        \n",
    "        # Update matched counts\n",
    "        total_matched += len(matched_aggregated_df)\n",
    "        total_unmatched_internal = len(final_unmatched_internal)\n",
    "        total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "# Then proceed with your existing Step 6 summary\n",
    "\n",
    "# 6. Summary of Reconciliation\n",
    "total_matched = len(matched_transactions) + len(matched_with_tolerance_df)\n",
    "total_unmatched_internal = len(final_unmatched_internal)\n",
    "total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "print(f\"\\nReconciliation Summary (Verto Nigeria):\")\n",
    "print(f\"Total Internal Credit Records: {len(verto_ng_hex_df_recon)}\")\n",
    "print(f\"Total Bank Credit Records: {len(verto_ng_bank_df_recon)}\")\n",
    "print(f\"Exact Matched Transactions: {len(matched_transactions)}\")\n",
    "print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched}\")\n",
    "print(f\"Remaining Unmatched Internal Records: {total_unmatched_internal}\")\n",
    "print(f\"Remaining Unmatched Bank Records: {total_unmatched_bank}\")\n",
    "\n",
    "if not matched_with_tolerance_df.empty:\n",
    "    print(\"\\nTransactions Matched with Date Tolerance:\")\n",
    "    print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', \n",
    "                                   'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\nUnmatched Internal Records:\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount_Internal']].head())\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\nUnmatched Bank Records:\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount_Bank']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6667b",
   "metadata": {},
   "source": [
    "## Fincra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e11e1a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Fincra NG): 2,586,500,000.00\n",
      "Total Bank Statement Credit Amount (Fincra NG): 2,586,525,000.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits): -25,000.00\n",
      "\n",
      "Reconciliation Summary (Fincra Nigeria):\n",
      "Total Internal Credit Records: 5\n",
      "Total Bank Credit Records: 6\n",
      "Exact Matched Transactions: 0\n",
      "Transactions Matched with Date Tolerance: 0\n",
      "Total Matched Transactions: 0\n",
      "Remaining Unmatched Internal Records: 5\n",
      "Remaining Unmatched Bank Records: 6\n",
      "\n",
      "Unmatched Internal Records:\n",
      "   Date_Match  Amount_Internal\n",
      "0  2025-06-02     3.228000e+08\n",
      "1  2025-06-06     1.176000e+09\n",
      "2  2025-06-12     3.438000e+08\n",
      "3  2025-06-19     3.183000e+08\n",
      "4  2025-06-20     4.256000e+08\n",
      "\n",
      "Unmatched Bank Records:\n",
      "  Date_Match  Amount_Bank\n",
      "5        NaT      25000.0\n",
      "6        NaT  318300000.0\n",
      "7        NaT  322800000.0\n",
      "8        NaT  343800000.0\n",
      "9        NaT  425600000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the datasets\n",
    "fincra_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/Fincra Hex.csv')\n",
    "\n",
    "try:\n",
    "    fincra_ng_bank_df = pd.read_excel(\n",
    "        '/Users/gracegitau/Downloads/#Recon May/NGN/Fincra Bank.xlsx', \n",
    "        engine='openpyxl',\n",
    "        header=0\n",
    "    )\n",
    "except:\n",
    "    try:\n",
    "        fincra_ng_bank_df = pd.read_csv(\n",
    "            '/Users/gracegitau/Downloads/#Recon May/NGN/Fincra Bank.csv',\n",
    "            header=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error loading bank statement:\", str(e))\n",
    "        raise\n",
    "\n",
    "# 2. Preprocessing for internal_df (Internal Records)\n",
    "fincra_ng_hex_df.columns = fincra_ng_hex_df.columns.str.strip()\n",
    "\n",
    "fincra_ng_hex_df = fincra_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "fincra_ng_hex_df['Date'] = pd.to_datetime(fincra_ng_hex_df['Date'])\n",
    "fincra_ng_hex_df_recon = fincra_ng_hex_df[fincra_ng_hex_df['Amount'] > 0].copy()\n",
    "fincra_ng_hex_df_recon = fincra_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "fincra_ng_hex_df_recon.loc[:, 'Date_Match'] = fincra_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# 3. Preprocessing for bank_df (Bank Statements) - FINCRA SPECIFIC\n",
    "fincra_ng_bank_df.columns = fincra_ng_bank_df.columns.str.strip()\n",
    "\n",
    "fincra_ng_bank_df = fincra_ng_bank_df.rename(columns={\n",
    "    'Date Initiated': 'Date',\n",
    "    'Amount Received': 'Credit',\n",
    "    'Reference': 'Transaction_ID'\n",
    "})\n",
    "\n",
    "fincra_ng_bank_df['Date'] = pd.to_datetime(\n",
    "    fincra_ng_bank_df['Date'], \n",
    "    format='%d/%m/%Y, %I:%M:%S %p GMT%z',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "fincra_ng_bank_df = fincra_ng_bank_df[\n",
    "    fincra_ng_bank_df['Status'].str.lower() == 'approved'\n",
    "].copy()\n",
    "\n",
    "fincra_ng_bank_df['Credit'] = (\n",
    "    fincra_ng_bank_df['Credit'].astype(str)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .replace('', '0')\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "fincra_ng_bank_df = fincra_ng_bank_df[fincra_ng_bank_df['Credit'] > 0].copy()\n",
    "fincra_ng_bank_df['Amount'] = fincra_ng_bank_df['Credit']\n",
    "fincra_ng_bank_df_recon = fincra_ng_bank_df[['Date', 'Amount', 'Transaction_ID']].copy()\n",
    "fincra_ng_bank_df_recon.loc[:, 'Date_Match'] = fincra_ng_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# 4. Calculate Total Amounts and Discrepancy\n",
    "total_internal_credits = fincra_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = fincra_ng_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Fincra NG): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Fincra NG): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits): {discrepancy_amount:,.2f}\")\n",
    "\n",
    "# 5. Reconciliation (transaction-level) with Date Tolerance\n",
    "# Ensure Date_Match columns have consistent types (convert both to string)\n",
    "fincra_ng_hex_df_recon['Date_Match'] = fincra_ng_hex_df_recon['Date_Match'].astype(str)\n",
    "fincra_ng_bank_df_recon['Date_Match'] = fincra_ng_bank_df_recon['Date_Match'].astype(str)\n",
    "\n",
    "# Round amounts to 2 decimal places for better matching accuracy\n",
    "fincra_ng_hex_df_recon['Amount_Rounded'] = fincra_ng_hex_df_recon['Amount'].round(2)\n",
    "fincra_ng_bank_df_recon['Amount_Rounded'] = fincra_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "reconciled_df = pd.merge(\n",
    "    fincra_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    fincra_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "\n",
    "unmatched_internal['Date_Match_DT'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "unmatched_bank['Date_Match_DT'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "\n",
    "matched_with_tolerance = []\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for idx, internal_row in unmatched_internal.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "    \n",
    "    potential_matches = unmatched_bank[\n",
    "        (unmatched_bank['Amount_Rounded'] == internal_amount) &\n",
    "        (abs(unmatched_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "    ]\n",
    "    \n",
    "    if not potential_matches.empty:\n",
    "        bank_match = potential_matches.iloc[0]\n",
    "        matched_with_tolerance.append({\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Date_Bank': bank_match['Date_Bank'],\n",
    "            'Amount_Bank': bank_match['Amount_Bank'],\n",
    "            'Date_Match_Bank': bank_match['Date_Match'],\n",
    "            'Amount_Rounded': internal_amount\n",
    "        })\n",
    "        matched_internal_indices.append(idx)\n",
    "        matched_bank_indices.append(bank_match.name)\n",
    "\n",
    "matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "final_unmatched_internal = unmatched_internal.drop(matched_internal_indices)\n",
    "final_unmatched_bank = unmatched_bank.drop(matched_bank_indices)\n",
    "\n",
    "# 6. Summary of Reconciliation\n",
    "total_matched = len(matched_transactions) + len(matched_with_tolerance_df)\n",
    "total_unmatched_internal = len(final_unmatched_internal)\n",
    "total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "print(f\"\\nReconciliation Summary (Fincra Nigeria):\")\n",
    "print(f\"Total Internal Credit Records: {len(fincra_ng_hex_df_recon)}\")\n",
    "print(f\"Total Bank Credit Records: {len(fincra_ng_bank_df_recon)}\")\n",
    "print(f\"Exact Matched Transactions: {len(matched_transactions)}\")\n",
    "print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched}\")\n",
    "print(f\"Remaining Unmatched Internal Records: {total_unmatched_internal}\")\n",
    "print(f\"Remaining Unmatched Bank Records: {total_unmatched_bank}\")\n",
    "\n",
    "if not matched_with_tolerance_df.empty:\n",
    "    print(\"\\nTransactions Matched with Date Tolerance:\")\n",
    "    print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', \n",
    "                                   'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\nUnmatched Internal Records:\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount_Internal']].head())\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\nUnmatched Bank Records:\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount_Bank']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65ef49",
   "metadata": {},
   "source": [
    "## Zenith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "79852a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Zenith NG): 932,400,000.00\n",
      "Total Bank Statement Credit Amount (Zenith NG): 932,800,000.00\n",
      "Overall Discrepancy (Internal Credits - Bank Credits): -400,000.00\n",
      "\n",
      "Reconciliation Summary (Zenith Nigeria):\n",
      "Total Internal Credit Records: 5\n",
      "Total Bank Credit Records: 5\n",
      "Exact Matched Transactions: 3\n",
      "Transactions Matched with Date Tolerance: 0\n",
      "Total Matched Transactions: 3\n",
      "Remaining Unmatched Internal Records: 2\n",
      "Remaining Unmatched Bank Records: 2\n",
      "\n",
      "Unmatched Internal Records:\n",
      "   Date_Match  Amount_Internal\n",
      "3  2025-06-13      154000000.0\n",
      "6  2025-06-19      154700000.0\n",
      "\n",
      "Unmatched Bank Records:\n",
      "   Date_Match  Amount_Bank\n",
      "4  2025-06-13  154500000.0\n",
      "5  2025-06-19  154600000.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the datasets\n",
    "zenith_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/Zenith Hex.csv')\n",
    "zenith_ng_bank_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/Zenith Bank.csv', header=3)\n",
    "\n",
    "# 2. Preprocessing for internal_df (Internal Records)\n",
    "zenith_ng_hex_df.columns = zenith_ng_hex_df.columns.str.strip()\n",
    "\n",
    "zenith_ng_hex_df = zenith_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "zenith_ng_hex_df['Date'] = pd.to_datetime(zenith_ng_hex_df['Date'])\n",
    "zenith_ng_hex_df_recon = zenith_ng_hex_df[zenith_ng_hex_df['Amount'] > 0].copy()\n",
    "zenith_ng_hex_df_recon = zenith_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "zenith_ng_hex_df_recon['Date_Match'] = zenith_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# 3. Preprocessing for bank_df (Bank Statements) - ZENITH SPECIFIC\n",
    "zenith_ng_bank_df.columns = zenith_ng_bank_df.columns.str.strip()\n",
    "\n",
    "# First remove any summary rows that don't contain proper dates\n",
    "zenith_ng_bank_df = zenith_ng_bank_df[\n",
    "    zenith_ng_bank_df['Effective Date'].str.match(r'\\d{2}/\\d{2}/\\d{4}', na=False)\n",
    "].copy()\n",
    "\n",
    "zenith_ng_bank_df = zenith_ng_bank_df.rename(columns={\n",
    "    'Effective Date': 'Date',\n",
    "    'Description/Payee/Memo': 'Description',\n",
    "    'Credit Amount': 'Credit'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime - Nigerian format (day/month/year)\n",
    "zenith_ng_bank_df['Date'] = pd.to_datetime(zenith_ng_bank_df['Date'], dayfirst=True)\n",
    "\n",
    "# Zenith Nigeria Specific Filters\n",
    "# 1. Filter for transactions with Description containing 'TRF'\n",
    "zenith_ng_bank_df = zenith_ng_bank_df[\n",
    "    zenith_ng_bank_df['Description'].str.contains('TRF FRM NALA PAYMENTS', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "# 2. Process Credit Amount - remove commas and convert to numeric\n",
    "zenith_ng_bank_df['Credit'] = (\n",
    "    zenith_ng_bank_df['Credit'].astype(str)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .replace('', '0')\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# 3. Filter for positive credits only\n",
    "zenith_ng_bank_df = zenith_ng_bank_df[\n",
    "    zenith_ng_bank_df['Credit'] > 0\n",
    "].copy()\n",
    "\n",
    "zenith_ng_bank_df['Amount'] = zenith_ng_bank_df['Credit']\n",
    "zenith_ng_bank_df_recon = zenith_ng_bank_df[['Date', 'Amount', 'Description']].copy()\n",
    "zenith_ng_bank_df_recon['Date_Match'] = zenith_ng_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# 4. Calculate Total Amounts and Discrepancy\n",
    "total_internal_credits = zenith_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = zenith_ng_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Zenith NG): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Zenith NG): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy (Internal Credits - Bank Credits): {discrepancy_amount:,.2f}\")\n",
    "\n",
    "# 5. Reconciliation (transaction-level) with Date Tolerance\n",
    "# Ensure Date_Match columns have consistent types\n",
    "zenith_ng_hex_df_recon['Date_Match'] = zenith_ng_hex_df_recon['Date_Match'].astype(str)\n",
    "zenith_ng_bank_df_recon['Date_Match'] = zenith_ng_bank_df_recon['Date_Match'].astype(str)\n",
    "\n",
    "zenith_ng_hex_df_recon['Amount_Rounded'] = zenith_ng_hex_df_recon['Amount'].round(2)\n",
    "zenith_ng_bank_df_recon['Amount_Rounded'] = zenith_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "reconciled_df = pd.merge(\n",
    "    zenith_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    zenith_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "\n",
    "unmatched_internal['Date_Match_DT'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "unmatched_bank['Date_Match_DT'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "\n",
    "matched_with_tolerance = []\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for idx, internal_row in unmatched_internal.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "    \n",
    "    potential_matches = unmatched_bank[\n",
    "        (unmatched_bank['Amount_Rounded'] == internal_amount) &\n",
    "        (abs(unmatched_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "    ]\n",
    "    \n",
    "    if not potential_matches.empty:\n",
    "        bank_match = potential_matches.iloc[0]\n",
    "        matched_with_tolerance.append({\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Date_Bank': bank_match['Date_Bank'],\n",
    "            'Amount_Bank': bank_match['Amount_Bank'],\n",
    "            'Date_Match_Bank': bank_match['Date_Match'],\n",
    "            'Amount_Rounded': internal_amount\n",
    "        })\n",
    "        matched_internal_indices.append(idx)\n",
    "        matched_bank_indices.append(bank_match.name)\n",
    "\n",
    "matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "final_unmatched_internal = unmatched_internal.drop(matched_internal_indices)\n",
    "final_unmatched_bank = unmatched_bank.drop(matched_bank_indices)\n",
    "\n",
    "# 6. Summary of Reconciliation\n",
    "total_matched = len(matched_transactions) + len(matched_with_tolerance_df)\n",
    "total_unmatched_internal = len(final_unmatched_internal)\n",
    "total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "print(f\"\\nReconciliation Summary (Zenith Nigeria):\")\n",
    "print(f\"Total Internal Credit Records: {len(zenith_ng_hex_df_recon)}\")\n",
    "print(f\"Total Bank Credit Records: {len(zenith_ng_bank_df_recon)}\")\n",
    "print(f\"Exact Matched Transactions: {len(matched_transactions)}\")\n",
    "print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched}\")\n",
    "print(f\"Remaining Unmatched Internal Records: {total_unmatched_internal}\")\n",
    "print(f\"Remaining Unmatched Bank Records: {total_unmatched_bank}\")\n",
    "\n",
    "if not matched_with_tolerance_df.empty:\n",
    "    print(\"\\nTransactions Matched with Date Tolerance:\")\n",
    "    print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', \n",
    "                                   'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\nUnmatched Internal Records:\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount_Internal']].head())\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\nUnmatched Bank Records:\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount_Bank']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3fce4",
   "metadata": {},
   "source": [
    "## FW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7f345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Internal Credit Amount (Flutterwave NG): 10,550,875,000.00\n",
      "Total Bank Statement Credit Amount (Flutterwave NG): 10,550,875,000.00\n",
      "Overall Discrepancy: 0.00\n",
      "\n",
      "Additional Matches Found After Aggregating Same-Day Internal Transactions:\n",
      "   Date_Match  Aggregated_Internal_Amount   Bank_Amount   Bank_Date\n",
      "0  2025-06-04                9.528000e+08  9.528000e+08  2025-06-04\n",
      "1  2025-06-05                2.143000e+09  2.143000e+09  2025-06-05\n",
      "2  2025-06-13                9.408000e+08  9.408000e+08  2025-06-13\n",
      "\n",
      "Reconciliation Summary (Flutterwave Nigeria):\n",
      "Total Internal Credit Records: 20\n",
      "Total Bank Credit Records: 17\n",
      "Exact Matched Transactions: 14\n",
      "Transactions Matched with Date Tolerance: 0\n",
      "Total Matched Transactions: 14\n",
      "Remaining Unmatched Internal Records: 0\n",
      "Remaining Unmatched Bank Records: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the datasets\n",
    "FW_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/FW Hex.csv')\n",
    "FW_ng_bank_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/FW Bank.csv', header=0)\n",
    "\n",
    "# 2. Preprocessing for internal_df (Internal Records)\n",
    "FW_ng_hex_df.columns = FW_ng_hex_df.columns.str.strip()\n",
    "\n",
    "FW_ng_hex_df = FW_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "FW_ng_hex_df['Date'] = pd.to_datetime(FW_ng_hex_df['Date'])\n",
    "FW_ng_hex_df_recon = FW_ng_hex_df[FW_ng_hex_df['Amount'] > 0].copy()\n",
    "FW_ng_hex_df_recon = FW_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "FW_ng_hex_df_recon['Date_Match'] = FW_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# 3. Preprocessing for bank_df (Bank Statements) - FLUTTERWAVE SPECIFIC\n",
    "FW_ng_bank_df.columns = FW_ng_bank_df.columns.str.strip()\n",
    "\n",
    "# Filter for Credits section (type='C') and exclude reversals\n",
    "FW_ng_bank_df = FW_ng_bank_df[\n",
    "    (FW_ng_bank_df['type'] == 'C') & \n",
    "    (~FW_ng_bank_df['remarks'].str.contains('rvsl', case=False, na=False))\n",
    "].copy()\n",
    "\n",
    "# Rename columns for consistency\n",
    "FW_ng_bank_df = FW_ng_bank_df.rename(columns={\n",
    "    'date': 'Date',\n",
    "    'amount': 'Credit',\n",
    "    'reference': 'Transaction_ID',\n",
    "    'remarks': 'Description'\n",
    "})\n",
    "\n",
    "# Convert 'Date' to datetime and remove timezone\n",
    "FW_ng_bank_df['Date'] = pd.to_datetime(FW_ng_bank_df['Date']).dt.tz_localize(None)\n",
    "\n",
    "# Process Credit Amount - ensure numeric format\n",
    "FW_ng_bank_df['Credit'] = (\n",
    "    FW_ng_bank_df['Credit'].astype(str)\n",
    "    .str.replace(',', '', regex=False)\n",
    "    .replace('', '0')\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Filter for positive credits only\n",
    "FW_ng_bank_df = FW_ng_bank_df[FW_ng_bank_df['Credit'] > 0].copy()\n",
    "\n",
    "# For reconciliation, use the credit values\n",
    "FW_ng_bank_df['Amount'] = FW_ng_bank_df['Credit']\n",
    "\n",
    "# Select relevant columns for reconciliation\n",
    "FW_ng_bank_df_recon = FW_ng_bank_df[['Date', 'Amount', 'Description', 'Transaction_ID']].copy()\n",
    "\n",
    "# Extract only the date component for matching\n",
    "FW_ng_bank_df_recon['Date_Match'] = FW_ng_bank_df_recon['Date'].dt.date\n",
    "\n",
    "# 4. Calculate Total Amounts and Discrepancy\n",
    "total_internal_credits = FW_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = FW_ng_bank_df_recon['Amount'].sum()\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"Total Internal Credit Amount (Flutterwave NG): {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount (Flutterwave NG): {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy: {discrepancy_amount:,.2f}\")\n",
    "\n",
    "# 5. Reconciliation (transaction-level) with Date Tolerance\n",
    "# Ensure Date_Match columns have consistent types\n",
    "FW_ng_hex_df_recon['Date_Match'] = FW_ng_hex_df_recon['Date_Match'].astype(str)\n",
    "FW_ng_bank_df_recon['Date_Match'] = FW_ng_bank_df_recon['Date_Match'].astype(str)\n",
    "\n",
    "FW_ng_hex_df_recon['Amount_Rounded'] = FW_ng_hex_df_recon['Amount'].round(2)\n",
    "FW_ng_bank_df_recon['Amount_Rounded'] = FW_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "reconciled_df = pd.merge(\n",
    "    FW_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "    FW_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "    on=['Date_Match', 'Amount_Rounded'],\n",
    "    how='outer',\n",
    "    suffixes=('_Internal', '_Bank')\n",
    ")\n",
    "\n",
    "matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "date_tolerance = pd.Timedelta(days=3)\n",
    "unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "\n",
    "unmatched_internal['Date_Match_DT'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "unmatched_bank['Date_Match_DT'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "\n",
    "matched_with_tolerance = []\n",
    "matched_internal_indices = []\n",
    "matched_bank_indices = []\n",
    "\n",
    "for idx, internal_row in unmatched_internal.iterrows():\n",
    "    internal_date = internal_row['Date_Match_DT']\n",
    "    internal_amount = internal_row['Amount_Rounded']\n",
    "    \n",
    "    potential_matches = unmatched_bank[\n",
    "        (unmatched_bank['Amount_Rounded'] == internal_amount) &\n",
    "        (abs(unmatched_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "    ]\n",
    "    \n",
    "    if not potential_matches.empty:\n",
    "        bank_match = potential_matches.iloc[0]\n",
    "        matched_with_tolerance.append({\n",
    "            'Date_Internal': internal_row['Date_Internal'],\n",
    "            'Amount_Internal': internal_row['Amount_Internal'],\n",
    "            'Date_Match_Internal': internal_row['Date_Match'],\n",
    "            'Date_Bank': bank_match['Date_Bank'],\n",
    "            'Amount_Bank': bank_match['Amount_Bank'],\n",
    "            'Date_Match_Bank': bank_match['Date_Match'],\n",
    "            'Amount_Rounded': internal_amount\n",
    "        })\n",
    "        matched_internal_indices.append(idx)\n",
    "        matched_bank_indices.append(bank_match.name)\n",
    "\n",
    "matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "final_unmatched_internal = unmatched_internal.drop(matched_internal_indices)\n",
    "final_unmatched_bank = unmatched_bank.drop(matched_bank_indices)\n",
    "\n",
    "# Merge same-day internal transactions before matching with bank\n",
    "if len(final_unmatched_internal) > 0:\n",
    "    # Group internal transactions by date and sum amounts\n",
    "    internal_aggregated = (\n",
    "        final_unmatched_internal.groupby('Date_Match')\n",
    "        .agg({'Amount_Internal': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "    internal_aggregated['Amount_Rounded'] = internal_aggregated['Amount_Internal'].round(2)\n",
    "    internal_aggregated['Date_Match_DT'] = pd.to_datetime(internal_aggregated['Date_Match'])\n",
    "    \n",
    "    # Try matching aggregated internal transactions with bank\n",
    "    bank_unmatched = final_unmatched_bank.copy()\n",
    "    bank_unmatched['Date_Match_DT'] = pd.to_datetime(bank_unmatched['Date_Match'])\n",
    "    \n",
    "    matched_aggregated = []\n",
    "    matched_bank_indices = []\n",
    "    \n",
    "    for idx, internal_row in internal_aggregated.iterrows():\n",
    "        internal_date = internal_row['Date_Match_DT']\n",
    "        internal_amount = internal_row['Amount_Rounded']\n",
    "        \n",
    "        potential_matches = bank_unmatched[\n",
    "            (bank_unmatched['Amount_Rounded'] == internal_amount) &\n",
    "            (bank_unmatched['Date_Match_DT'] == internal_date)\n",
    "        ]\n",
    "        \n",
    "        if not potential_matches.empty:\n",
    "            bank_match = potential_matches.iloc[0]\n",
    "            matched_aggregated.append({\n",
    "                'Date_Match': internal_row['Date_Match'],\n",
    "                'Aggregated_Internal_Amount': internal_row['Amount_Internal'],\n",
    "                'Bank_Amount': bank_match['Amount_Bank'],\n",
    "                'Bank_Date': bank_match['Date_Match']\n",
    "            })\n",
    "            matched_bank_indices.append(bank_match.name)\n",
    "    \n",
    "    # Update unmatched records\n",
    "    if matched_aggregated:\n",
    "        matched_aggregated_df = pd.DataFrame(matched_aggregated)\n",
    "        final_unmatched_bank = final_unmatched_bank.drop(matched_bank_indices)\n",
    "        \n",
    "        # Remove the matched internal transactions\n",
    "        matched_dates = matched_aggregated_df['Date_Match'].unique()\n",
    "        final_unmatched_internal = final_unmatched_internal[\n",
    "            ~final_unmatched_internal['Date_Match'].isin(matched_dates)\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nAdditional Matches Found After Aggregating Same-Day Internal Transactions:\")\n",
    "        print(matched_aggregated_df)\n",
    "        \n",
    "        # Update matched counts\n",
    "        total_matched += len(matched_aggregated_df)\n",
    "        total_unmatched_internal = len(final_unmatched_internal)\n",
    "        total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "\n",
    "# 6. Summary of Reconciliation\n",
    "total_matched = len(matched_transactions) + len(matched_with_tolerance_df)\n",
    "total_unmatched_internal = len(final_unmatched_internal)\n",
    "total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "print(f\"\\nReconciliation Summary:\")\n",
    "print(f\"Total Internal Credit Records: {len(FW_ng_hex_df_recon)}\")\n",
    "print(f\"Total Bank Credit Records: {len(FW_ng_bank_df_recon)}\")\n",
    "print(f\"Exact Matched Transactions: {len(matched_transactions)}\")\n",
    "print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "print(f\"Total Matched Transactions: {total_matched}\")\n",
    "print(f\"Remaining Unmatched Internal Records: {total_unmatched_internal}\")\n",
    "print(f\"Remaining Unmatched Bank Records: {total_unmatched_bank}\")\n",
    "\n",
    "if not matched_with_tolerance_df.empty:\n",
    "    print(\"\\nTransactions Matched with Date Tolerance:\")\n",
    "    print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', \n",
    "                                   'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "if not final_unmatched_internal.empty:\n",
    "    print(\"\\nUnmatched Internal Records:\")\n",
    "    print(final_unmatched_internal[['Date_Match', 'Amount_Internal']].head())\n",
    "\n",
    "if not final_unmatched_bank.empty:\n",
    "    print(\"\\nUnmatched Bank Records:\")\n",
    "    print(final_unmatched_bank[['Date_Match', 'Amount_Bank']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23226ea5",
   "metadata": {},
   "source": [
    "## MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0367daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3 sheets from /Users/gracegitau/Downloads/#Recon May/NGN/MP Bank.xls...\n",
      "Columns derived from first sheet (Monnify_Account_Statement): ['AMOUNT', 'BALANCE_BEFORE', 'BALANCE_AFTER', 'REFERENCE', 'DATE', 'TRANSACTION_TYPE', 'NARRATION']\n",
      "Shape of first sheet: (65535, 7)\n",
      "Appended sheet 'Monnify_Account_Statement 2' with shape (65536, 7)\n",
      "Appended sheet 'Monnify_Account_Statement 3' with shape (33805, 7)\n",
      "Successfully combined 3 sheets into a single DataFrame with shape: (164876, 7)\n",
      "\n",
      "Duplicate records found in Internal Records:\n",
      "ID: fx-transfer-2z36clGZ7MtbiYoLQTM77cK633P, Amount: 429800000.0, Date: 2025-06-26 00:00:00, Comment: fx-deal-quote-2z2jye54GJcgyUcTAq4cKtJIWkz\n",
      "ID: fx-transfer-2z39wSNPNxBrmMu8GYZiYeF5Bkd, Amount: 625200000.0, Date: 2025-06-26 00:00:00, Comment: fx-deal-quote-2z2jye54GJcgyUcTAq4cKtJIWkz\n",
      "\n",
      "Starting bank statement preprocessing with combined shape: (164876, 7)\n",
      "Columns BEFORE stripping: ['AMOUNT', 'BALANCE_BEFORE', 'BALANCE_AFTER', 'REFERENCE', 'DATE', 'TRANSACTION_TYPE', 'NARRATION']\n",
      "Columns AFTER stripping: ['AMOUNT', 'BALANCE_BEFORE', 'BALANCE_AFTER', 'REFERENCE', 'DATE', 'TRANSACTION_TYPE', 'NARRATION']\n",
      "Shape AFTER stripping: (164876, 7)\n",
      "Shape after date filtering (June): (159559, 7)\n",
      "Shape after TRANSACTION_TYPE, NARRATION, REFERENCE filters: (476, 7)\n",
      "Shape after grouping by SENDER_NAME and 30min window: (140, 6)\n",
      "\n",
      "Total Internal Credit Amount: 41,041,219,451.68\n",
      "Total Bank Statement Credit Amount: 40,365,339,018.68\n",
      "Overall Discrepancy : 675,880,433.00\n",
      "\n",
      "Columns in reconciled_df: ['Date', 'Amount_Internal', 'Description', 'ID', 'Date_Match', 'Amount_Rounded', 'Source_Internal', 'SENDER_NAME', 'DATE_TIME_WINDOW_START', 'Amount_Bank', 'NARRATION', 'REFERENCE', 'TRANSACTION_TYPE', 'Source_Bank']\n",
      "\n",
      "Columns in unmatched_internal: ['Date', 'Amount_Internal', 'Description', 'ID', 'Date_Match', 'Amount_Rounded', 'Source_Internal', 'SENDER_NAME', 'DATE_TIME_WINDOW_START', 'Amount_Bank', 'NARRATION', 'REFERENCE', 'TRANSACTION_TYPE', 'Source_Bank']\n",
      "Columns in unmatched_bank: ['Date', 'Amount_Internal', 'Description', 'ID', 'Date_Match', 'Amount_Rounded', 'Source_Internal', 'SENDER_NAME', 'DATE_TIME_WINDOW_START', 'Amount_Bank', 'NARRATION', 'REFERENCE', 'TRANSACTION_TYPE', 'Source_Bank']\n",
      "\n",
      "Reconciliation Summary:\n",
      "Total Internal Credit Records: 105\n",
      "Total Bank Credit Records: 140\n",
      "Exact Matched Transactions: 61\n",
      "Transactions Matched with Date Tolerance: 1\n",
      "Total Matched Transactions: 62\n",
      "Remaining Unmatched Internal Records: 43\n",
      "Remaining Unmatched Bank Records: 78\n",
      "\n",
      "Transactions Matched with Date Tolerance:\n",
      "  Date_Match_Internal  Amount_Internal Date_Match_Bank  Amount_Bank\n",
      "0          2025-06-16         156500.0      2025-06-17     156500.0\n",
      "\n",
      "Unmatched Internal Records:\n",
      "    Date_Match  Amount_Internal\n",
      "3   2025-06-02      413660000.0\n",
      "11  2025-06-03      361900000.0\n",
      "13  2025-06-03      634400000.0\n",
      "16  2025-06-04      428400000.0\n",
      "25  2025-06-05      284760000.0\n",
      "\n",
      "Unmatched Bank Records:\n",
      "    Date_Match  Amount_Bank\n",
      "1   2025-06-02  319899356.0\n",
      "5   2025-06-03     400000.0\n",
      "8   2025-06-03  200000000.0\n",
      "12  2025-06-03  634000000.0\n",
      "14  2025-06-04  100000000.0\n",
      "\n",
      "Analyzing unmatched transactions with clustering...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Amount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Amount'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 300\u001b[0m\n\u001b[1;32m    297\u001b[0m final_unmatched_bank[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSENDER_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_unmatched_bank[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNARRATION\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_sender_name)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Run clustering analysis\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m clusters_df, discrepancies_df \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_unmatched_transactions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_unmatched_internal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_unmatched_bank\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clusters_df\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[0;32mIn[158], line 28\u001b[0m, in \u001b[0;36mcluster_unmatched_transactions\u001b[0;34m(unmatched_internal, unmatched_bank, date_tol)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, int_row \u001b[38;5;129;01min\u001b[39;00m internal\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     27\u001b[0m     int_date \u001b[38;5;241m=\u001b[39m int_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_Match\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     int_amount \u001b[38;5;241m=\u001b[39m \u001b[43mint_row\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAmount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Find bank transactions within date tolerance\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     bank_matches \u001b[38;5;241m=\u001b[39m bank[\n\u001b[1;32m     32\u001b[0m         (\u001b[38;5;28mabs\u001b[39m((bank[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate_Match\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m int_date)\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m date_tol) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m     33\u001b[0m         (\u001b[38;5;241m~\u001b[39mbank\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39misin(used_bank_indices)))\n\u001b[1;32m     34\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Amount'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the datasets - MoniePoint specific multi-sheet handling with header fix\n",
    "def load_moniepoint_bank_statement(filepath):\n",
    "    try:\n",
    "        # Get all sheet names\n",
    "        xls = pd.ExcelFile(filepath, engine='xlrd')\n",
    "        sheet_names = xls.sheet_names\n",
    "        \n",
    "        if not sheet_names:\n",
    "            print(\"No sheets found in the Excel file.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"Loading {len(sheet_names)} sheets from {filepath}...\")\n",
    "\n",
    "        # Read the first sheet with header\n",
    "        df_first_sheet = pd.read_excel(filepath, sheet_name=sheet_names[0], engine='xlrd', header=0)\n",
    "        columns = df_first_sheet.columns.tolist() # Get columns from the first sheet\n",
    "        print(f\"Columns derived from first sheet ({sheet_names[0]}): {columns}\")\n",
    "        print(f\"Shape of first sheet: {df_first_sheet.shape}\")\n",
    "\n",
    "\n",
    "        # Store all processed DataFrames\n",
    "        all_dfs = [df_first_sheet]\n",
    "\n",
    "        # Read the remaining sheets without header and assign columns\n",
    "        for i in range(1, len(sheet_names)):\n",
    "            sheet_name = sheet_names[i]\n",
    "            # Read without header, then assign columns from the first sheet\n",
    "            df_other_sheet = pd.read_excel(filepath, sheet_name=sheet_name, engine='xlrd', header=None)\n",
    "            \n",
    "            # Ensure the number of columns matches before assigning\n",
    "            if df_other_sheet.shape[1] == len(columns):\n",
    "                df_other_sheet.columns = columns\n",
    "                all_dfs.append(df_other_sheet)\n",
    "                print(f\"Appended sheet '{sheet_name}' with shape {df_other_sheet.shape}\")\n",
    "            else:\n",
    "                print(f\"Warning: Sheet '{sheet_name}' has {df_other_sheet.shape[1]} columns, expected {len(columns)}. Skipping due to column mismatch.\")\n",
    "                # You might want to handle this differently, e.g., inspect the sheet\n",
    "                # or try to force alignment if the mismatch is minor.\n",
    "                # For now, we'll skip sheets that don't match the column count.\n",
    "\n",
    "        # Combine all sheets into one DataFrame\n",
    "        df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        print(f\"Successfully combined {len(all_dfs)} sheets into a single DataFrame with shape: {df_combined.shape}\")\n",
    "        return df_combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MoniePoint bank statement: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load internal records and bank statements\n",
    "MP_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/MP Hex.csv')\n",
    "MP_ng_bank_df = load_moniepoint_bank_statement('/Users/gracegitau/Downloads/#Recon May/NGN/MP Bank.xls')\n",
    "\n",
    "# 2. Preprocessing for internal_df (Internal Records)\n",
    "MP_ng_hex_df.columns = MP_ng_hex_df.columns.str.strip()\n",
    "\n",
    "MP_ng_hex_df = MP_ng_hex_df.rename(columns={\n",
    "    'TRANSFER_DATE': 'Date',\n",
    "    'AMOUNT': 'Amount',\n",
    "    'COMMENT': 'Description',\n",
    "    'TRANSFER_ID': 'ID'\n",
    "})\n",
    "\n",
    "MP_ng_hex_df['Date'] = pd.to_datetime(MP_ng_hex_df['Date'])\n",
    "MP_ng_hex_df_recon = MP_ng_hex_df[MP_ng_hex_df['Amount'] > 0].copy()\n",
    "MP_ng_hex_df_recon = MP_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "MP_ng_hex_df_recon['Date_Match'] = MP_ng_hex_df_recon['Date'].dt.date\n",
    "\n",
    "# Check for duplicates in internal records\n",
    "duplicate_internal_records = MP_ng_hex_df_recon[MP_ng_hex_df_recon.duplicated(subset=['Description'], keep=False)]\n",
    "if not duplicate_internal_records.empty:\n",
    "    print(\"\\nDuplicate records found in Internal Records:\")\n",
    "    for index, row in duplicate_internal_records.iterrows():\n",
    "        print(f\"ID: {row['ID']}, Amount: {row['Amount']}, Date: {row['Date']}, Comment: {row['Description']}\")\n",
    "else:\n",
    "    print(\"\\nNo duplicate records found in Internal Records.\")\n",
    "\n",
    "# 3. Preprocessing for bank_df (Bank Statements) - MONIEPOINT SPECIFIC\n",
    "# This function will now receive the *already correctly combined* DataFrame\n",
    "def preprocess_moniepoint_bank(df):\n",
    "    print(f\"\\nStarting bank statement preprocessing with combined shape: {df.shape}\")\n",
    "    print(f\"Columns BEFORE stripping: {df.columns.tolist()}\") # DEBUG\n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    print(f\"Columns AFTER stripping: {df.columns.tolist()}\") # DEBUG\n",
    "    print(f\"Shape AFTER stripping: {df.shape}\") # DEBUG\n",
    "\n",
    "    # Ensure 'DATE' column exists and is datetime\n",
    "    if 'DATE' not in df.columns:\n",
    "        print(\"Error: 'DATE' column not found in the bank statement. Cannot proceed with date filtering.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce') # 'coerce' turns unparseable dates into NaT\n",
    "    df = df.dropna(subset=['DATE']).copy() # Drop rows where DATE conversion failed\n",
    "    \n",
    "    # Filter for current month (June)\n",
    "    df = df[df['DATE'].dt.month == 6].copy()\n",
    "    print(f\"Shape after date filtering (June): {df.shape}\")\n",
    "    \n",
    "    # Apply filters\n",
    "    # Ensure all required columns exist before filtering\n",
    "    required_cols_for_filters = ['TRANSACTION_TYPE', 'NARRATION', 'REFERENCE', 'AMOUNT'] # Added 'AMOUNT' here for safety\n",
    "    if not all(col in df.columns for col in required_cols_for_filters):\n",
    "        missing = [col for col in required_cols_for_filters if col not in df.columns]\n",
    "        print(f\"Error: Missing required columns for filtering: {missing}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_cleaned = df[\n",
    "        (df['TRANSACTION_TYPE'] == 'CREDIT') & \n",
    "        (df['NARRATION'].str.contains('MFY-WT', na=False)) & \n",
    "        (~df['REFERENCE'].str.contains('RVSL', na=False))\n",
    "    ].copy()\n",
    "    print(f\"Shape after TRANSACTION_TYPE, NARRATION, REFERENCE filters: {df_cleaned.shape}\")\n",
    "\n",
    "    if df_cleaned.empty:\n",
    "        print(\"No records found after applying all bank statement filters. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract sender name from narration\n",
    "    def extract_sender_name(narration):\n",
    "        narration = str(narration).lower()\n",
    "        if 'verto financial tech' in narration or 'paga' in narration:\n",
    "            return 'VertoFX, NGN a/c'\n",
    "        elif 'sendfirst' in narration or 'duplo ltd' in narration:\n",
    "            return 'Duplo, NGN a/c'\n",
    "        elif 'esca' in narration:\n",
    "            return 'Esca Nigeria, NGN a/c'\n",
    "        elif 'resrv' in narration:\n",
    "            return 'Resrv FX, NGN a/c'\n",
    "        elif 'waza' in narration:\n",
    "            return 'Waza, Nigeria, NGN a/c'\n",
    "        elif 'flutterwave' in narration:\n",
    "            return 'Flutterwave, NGN a/c'\n",
    "        elif 'inexass' in narration:\n",
    "            return 'AZA Finance, NGN a/c'\n",
    "        elif 'nala' in narration:\n",
    "            return 'Nala Payments'\n",
    "        elif 'south one' in narration:\n",
    "            return 'Southone NGN a/c'\n",
    "        elif 'titan-paystack' in narration or 'multigate' in narration:\n",
    "            return 'Multigate, NGN a/c'\n",
    "        elif 'zerozilo' in narration or 'silverfile' in narration or 'palm bills' in narration:\n",
    "            return 'Fincra, NGN a/c'\n",
    "        elif 'ift technologies' in narration or 'budpay' in narration or 'bud infrastructure' in narration:\n",
    "            return 'Torus Mara, NGN a/c'\n",
    "        elif 'starks associates limited' in narration or 'shamiri' in narration or 'second jeu' in narration:\n",
    "            return 'Straitpay (Starks), UK, NGN a/c'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    df_cleaned['SENDER_NAME'] = df_cleaned['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    # Combine split transactions within 30-minute windows\n",
    "    # Ensure 'AMOUNT' column exists before grouping\n",
    "    if 'AMOUNT' not in df_cleaned.columns:\n",
    "        print(\"Error: 'AMOUNT' column not found for aggregation. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_combined = df_cleaned.groupby(['SENDER_NAME', pd.Grouper(key='DATE', freq='30min')]).agg({\n",
    "        'AMOUNT': 'sum',\n",
    "        'NARRATION': lambda x: ' '.join(x.dropna().unique()),\n",
    "        'REFERENCE': lambda x: ' '.join(x.dropna().unique()),\n",
    "        'TRANSACTION_TYPE': 'first',\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_combined.rename(columns={'DATE': 'DATE_TIME_WINDOW_START', 'AMOUNT': 'Amount'}, inplace=True)\n",
    "    print(f\"Shape after grouping by SENDER_NAME and 30min window: {df_combined.shape}\")\n",
    "    return df_combined\n",
    "\n",
    "# Only proceed with preprocessing if the bank statement was loaded successfully\n",
    "if not MP_ng_bank_df.empty:\n",
    "    MP_ng_bank_df_recon = preprocess_moniepoint_bank(MP_ng_bank_df)\n",
    "    if not MP_ng_bank_df_recon.empty:\n",
    "        MP_ng_bank_df_recon['Date_Match'] = MP_ng_bank_df_recon['DATE_TIME_WINDOW_START'].dt.date\n",
    "    else:\n",
    "        print(\"MP_ng_bank_df_recon is empty after preprocessing.\")\n",
    "else:\n",
    "    MP_ng_bank_df_recon = pd.DataFrame()\n",
    "    print(\"MP_ng_bank_df is empty, skipping bank statement preprocessing.\")\n",
    "\n",
    "\n",
    "# 4. Calculate Total Amounts and Discrepancy\n",
    "total_internal_credits = MP_ng_hex_df_recon['Amount'].sum()\n",
    "total_bank_credits = MP_ng_bank_df_recon['Amount'].sum() if not MP_ng_bank_df_recon.empty else 0\n",
    "discrepancy_amount = total_internal_credits - total_bank_credits\n",
    "\n",
    "print(f\"\\nTotal Internal Credit Amount: {total_internal_credits:,.2f}\")\n",
    "print(f\"Total Bank Statement Credit Amount: {total_bank_credits:,.2f}\")\n",
    "print(f\"Overall Discrepancy : {discrepancy_amount:,.2f}\")\n",
    "\n",
    "# 5. Reconciliation (transaction-level) with Date Tolerance\n",
    "MP_ng_hex_df_recon['Amount_Rounded'] = MP_ng_hex_df_recon['Amount'].round(2)\n",
    "\n",
    "if not MP_ng_bank_df_recon.empty:\n",
    "    MP_ng_bank_df_recon['Amount_Rounded'] = MP_ng_bank_df_recon['Amount'].round(2)\n",
    "\n",
    "    # First try matching by amount and exact date\n",
    "    reconciled_df = pd.merge(\n",
    "        MP_ng_hex_df_recon.assign(Source_Internal='Internal'),\n",
    "        MP_ng_bank_df_recon.assign(Source_Bank='Bank'),\n",
    "        on=['Date_Match', 'Amount_Rounded'],\n",
    "        how='outer',\n",
    "        suffixes=('_Internal', '_Bank')\n",
    "    )\n",
    "    \n",
    "    # Debug prints\n",
    "    print(\"\\nColumns in reconciled_df:\", reconciled_df.columns.tolist())\n",
    "\n",
    "    matched_transactions = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "\n",
    "    date_tolerance = pd.Timedelta(days=3)\n",
    "    unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "    unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "\n",
    "    # Debug prints\n",
    "    print(\"\\nColumns in unmatched_internal:\", unmatched_internal.columns.tolist())\n",
    "    print(\"Columns in unmatched_bank:\", unmatched_bank.columns.tolist())\n",
    "\n",
    "\n",
    "    unmatched_internal['Date_Match_DT'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "    unmatched_bank['Date_Match_DT'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "\n",
    "    matched_with_tolerance = []\n",
    "    matched_internal_indices = []\n",
    "    matched_bank_indices = []\n",
    "\n",
    "    for idx, internal_row in unmatched_internal.iterrows():\n",
    "        internal_date = internal_row['Date_Match_DT']\n",
    "        internal_amount = internal_row['Amount_Rounded']\n",
    "        \n",
    "        potential_matches = unmatched_bank[\n",
    "            (unmatched_bank['Amount_Rounded'] == internal_amount) &\n",
    "            (abs(unmatched_bank['Date_Match_DT'] - internal_date) <= date_tolerance)\n",
    "        ]\n",
    "        \n",
    "        if not potential_matches.empty:\n",
    "            bank_match = potential_matches.iloc[0]\n",
    "            matched_with_tolerance.append({\n",
    "                # Corrected column names as discussed in the previous turn\n",
    "                'Date_Internal': internal_row['Date'], \n",
    "                'Amount_Internal': internal_row['Amount_Internal'], \n",
    "                'Description_Internal': internal_row['Description'], \n",
    "                'ID_Internal': internal_row['ID'],\n",
    "                \n",
    "                'Date_Match_Internal': internal_row['Date_Match'], \n",
    "\n",
    "                'Date_Bank': bank_match['DATE_TIME_WINDOW_START'], \n",
    "                'Amount_Bank': bank_match['Amount_Bank'], \n",
    "                \n",
    "                'Date_Match_Bank': bank_match['Date_Match'], \n",
    "                'Amount_Rounded': internal_amount\n",
    "            })\n",
    "            matched_internal_indices.append(idx)\n",
    "            matched_bank_indices.append(bank_match.name)\n",
    "\n",
    "    matched_with_tolerance_df = pd.DataFrame(matched_with_tolerance)\n",
    "    final_unmatched_internal = unmatched_internal.drop(matched_internal_indices, errors='ignore')\n",
    "    final_unmatched_bank = unmatched_bank.drop(matched_bank_indices, errors='ignore')\n",
    "\n",
    "    # 6. Summary of Reconciliation\n",
    "    total_matched = len(matched_transactions) + len(matched_with_tolerance_df)\n",
    "    total_unmatched_internal = len(final_unmatched_internal)\n",
    "    total_unmatched_bank = len(final_unmatched_bank)\n",
    "\n",
    "    print(f\"\\nReconciliation Summary:\")\n",
    "    print(f\"Total Internal Credit Records: {len(MP_ng_hex_df_recon)}\")\n",
    "    print(f\"Total Bank Credit Records: {len(MP_ng_bank_df_recon)}\")\n",
    "    print(f\"Exact Matched Transactions: {len(matched_transactions)}\")\n",
    "    print(f\"Transactions Matched with Date Tolerance: {len(matched_with_tolerance_df)}\")\n",
    "    print(f\"Total Matched Transactions: {total_matched}\")\n",
    "    print(f\"Remaining Unmatched Internal Records: {total_unmatched_internal}\")\n",
    "    print(f\"Remaining Unmatched Bank Records: {total_unmatched_bank}\")\n",
    "\n",
    "    if not matched_with_tolerance_df.empty:\n",
    "        print(\"\\nTransactions Matched with Date Tolerance:\")\n",
    "        print(matched_with_tolerance_df[['Date_Match_Internal', 'Amount_Internal', \n",
    "                                       'Date_Match_Bank', 'Amount_Bank']])\n",
    "\n",
    "    if not final_unmatched_internal.empty:\n",
    "        print(\"\\nUnmatched Internal Records:\")\n",
    "        print(final_unmatched_internal[['Date_Match', 'Amount_Internal']].head())\n",
    "\n",
    "    if not final_unmatched_bank.empty:\n",
    "        print(\"\\nUnmatched Bank Records:\")\n",
    "        print(final_unmatched_bank[['Date_Match', 'Amount_Bank']].head())\n",
    "else:\n",
    "    print(\"\\nSkipping reconciliation steps as no bank statement data was loaded or processed successfully.\")\n",
    "\n",
    "\n",
    "# After your existing matching logic, add this:\n",
    "if not final_unmatched_internal.empty and not final_unmatched_bank.empty:\n",
    "    print(\"\\nAnalyzing unmatched transactions with clustering...\")\n",
    "    \n",
    "    # Ensure we have the SENDER_NAME column in bank unmatched\n",
    "    final_unmatched_bank['SENDER_NAME'] = final_unmatched_bank['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    # Run clustering analysis\n",
    "    clusters_df, discrepancies_df = cluster_unmatched_transactions(\n",
    "        final_unmatched_internal,\n",
    "        final_unmatched_bank\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    if not clusters_df.empty:\n",
    "        print(\"\\nTransaction Clusters Summary:\")\n",
    "        print(clusters_df)\n",
    "        \n",
    "        if not discrepancies_df.empty:\n",
    "            print(\"\\nDiscrepancies Found:\")\n",
    "            print(discrepancies_df)\n",
    "            \n",
    "            # Calculate total discrepancies by type\n",
    "            under_payment = discrepancies_df[discrepancies_df['type'] == 'under']['difference'].sum()\n",
    "            over_payment = discrepancies_df[discrepancies_df['type'] == 'over']['difference'].sum()\n",
    "            \n",
    "            print(f\"\\nTotal Under-payment by Bank: {under_payment:,.2f}\")\n",
    "            print(f\"Total Over-payment by Bank: {abs(over_payment):,.2f}\")\n",
    "            \n",
    "            # Compare with original discrepancy\n",
    "            calculated_discrepancy = under_payment + over_payment\n",
    "            print(f\"\\nCalculated Discrepancy from Clusters: {calculated_discrepancy:,.2f}\")\n",
    "            print(f\"Original Discrepancy: {discrepancy_amount:,.2f}\")\n",
    "            print(f\"Unaccounted Difference: {discrepancy_amount - calculated_discrepancy:,.2f}\")\n",
    "    else:\n",
    "        print(\"No clusters found in unmatched transactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading Functions\n",
    "def load_moniepoint_bank_statement(filepath):\n",
    "    \"\"\"Load and combine multi-sheet MoniePoint bank statement\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(filepath, engine='xlrd')\n",
    "        sheet_names = xls.sheet_names\n",
    "        \n",
    "        if not sheet_names:\n",
    "            print(\"No sheets found in the Excel file.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"Loading {len(sheet_names)} sheets from {filepath}...\")\n",
    "\n",
    "        # Read first sheet to get columns\n",
    "        df_first_sheet = pd.read_excel(filepath, sheet_name=sheet_names[0], engine='xlrd', header=0)\n",
    "        columns = df_first_sheet.columns.tolist()\n",
    "        print(f\"Columns from first sheet ({sheet_names[0]}): {columns}\")\n",
    "\n",
    "        # Process all sheets\n",
    "        all_dfs = [df_first_sheet]\n",
    "        for sheet_name in sheet_names[1:]:\n",
    "            df_sheet = pd.read_excel(filepath, sheet_name=sheet_name, engine='xlrd', header=None)\n",
    "            if df_sheet.shape[1] == len(columns):\n",
    "                df_sheet.columns = columns\n",
    "                all_dfs.append(df_sheet)\n",
    "                print(f\"Appended {sheet_name} with shape {df_sheet.shape}\")\n",
    "            else:\n",
    "                print(f\"Warning: Skipping {sheet_name} - column mismatch\")\n",
    "\n",
    "        df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Combined shape: {df_combined.shape}\")\n",
    "        return df_combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading bank statement: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 2. Sender Identification Functions\n",
    "def extract_sender_name(narration):\n",
    "    \"\"\"Extract sender name from bank narration\"\"\"\n",
    "    narration = str(narration).lower()\n",
    "    if 'verto financial tech' in narration or 'paga' in narration:\n",
    "        return 'VertoFX, NGN a/c'\n",
    "    elif 'sendfirst' in narration or 'duplo ltd' in narration:\n",
    "        return 'Duplo, NGN a/c'\n",
    "    elif 'esca' in narration:\n",
    "        return 'Esca Nigeria, NGN a/c'\n",
    "    elif 'resrv' in narration:\n",
    "        return 'Resrv FX, NGN a/c'\n",
    "    elif 'waza' in narration:\n",
    "        return 'Waza, Nigeria, NGN a/c'\n",
    "    elif 'flutterwave' in narration:\n",
    "        return 'Flutterwave, NGN a/c'\n",
    "    elif 'inexass' in narration:\n",
    "        return 'AZA Finance, NGN a/c'\n",
    "    elif 'nala' in narration:\n",
    "        return 'Nala Payments'\n",
    "    elif 'south one' in narration:\n",
    "        return 'Southone NGN a/c'\n",
    "    elif 'titan-paystack' in narration or 'multigate' in narration:\n",
    "        return 'Multigate, NGN a/c'\n",
    "    elif 'zerozilo' in narration or 'silverfile' in narration or 'palm bills' in narration:\n",
    "        return 'Fincra, NGN a/c'\n",
    "    elif 'ift technologies' in narration or 'budpay' in narration or 'bud infrastructure' in narration:\n",
    "        return 'Torus Mara, NGN a/c'\n",
    "    elif 'starks associates limited' in narration or 'shamiri' in narration or 'second jeu' in narration:\n",
    "        return 'Straitpay (Starks), UK, NGN a/c'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def extract_internal_sender(description):\n",
    "    \"\"\"Extract sender name from internal description\"\"\"\n",
    "    description = str(description).lower()\n",
    "    if 'fx-deal' in description:\n",
    "        for sender in ['verto', 'duplo', 'esca', 'resrv', 'waza', 'flutterwave']:\n",
    "            if sender in description:\n",
    "                return f\"{sender.title()}, NGN a/c\"\n",
    "    elif 'payout' in description:\n",
    "        return 'Internal Payout'\n",
    "    return extract_sender_name(description)  # Fallback to bank patterns\n",
    "\n",
    "# 3. Bank Statement Preprocessing\n",
    "def preprocess_moniepoint_bank(df):\n",
    "    \"\"\"Clean and prepare bank statement data\"\"\"\n",
    "    print(f\"\\nStarting bank preprocessing. Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert and filter dates\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
    "    df = df.dropna(subset=['DATE']).copy()\n",
    "    df = df[df['DATE'].dt.month == 6].copy()  # Filter for June\n",
    "    \n",
    "    # Apply business rules\n",
    "    df_cleaned = df[\n",
    "        (df['TRANSACTION_TYPE'] == 'CREDIT') & \n",
    "        (df['NARRATION'].str.contains('MFY-WT', na=False)) & \n",
    "        (~df['REFERENCE'].str.contains('RVSL', na=False))\n",
    "    ].copy()\n",
    "    \n",
    "    if df_cleaned.empty:\n",
    "        print(\"No records after filters\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract sender and combine transactions\n",
    "    df_cleaned['SENDER_NAME'] = df_cleaned['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    df_combined = df_cleaned.groupby(['SENDER_NAME', pd.Grouper(key='DATE', freq='30min')]).agg({\n",
    "        'AMOUNT': 'sum',\n",
    "        'NARRATION': lambda x: ' '.join(x.dropna().unique()),\n",
    "        'REFERENCE': lambda x: ' '.join(x.dropna().unique()),\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_combined.rename(columns={'DATE': 'DATE_TIME_WINDOW_START', 'AMOUNT': 'Amount'}, inplace=True)\n",
    "    print(f\"Final bank shape: {df_combined.shape}\")\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d02e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_unmatched_transactions(unmatched_internal, unmatched_bank, date_tol=3):\n",
    "    \"\"\"Group unmatched transactions by sender and date proximity\"\"\"\n",
    "    # Ensure date columns are datetime\n",
    "    unmatched_internal['Date_Match'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "    unmatched_bank['Date_Match'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "    \n",
    "    # Add sender names\n",
    "    unmatched_internal['SENDER_NAME'] = unmatched_internal['Description'].apply(extract_internal_sender)\n",
    "    unmatched_bank['SENDER_NAME'] = unmatched_bank['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    clusters = []\n",
    "    discrepancies = []\n",
    "    \n",
    "    # Process by sender\n",
    "    for sender in set(unmatched_internal['SENDER_NAME'].unique()).union(set(unmatched_bank['SENDER_NAME'].unique())):\n",
    "        internal = unmatched_internal[unmatched_internal['SENDER_NAME'] == sender]\n",
    "        bank = unmatched_bank[unmatched_bank['SENDER_NAME'] == sender]\n",
    "        \n",
    "        if internal.empty and bank.empty:\n",
    "            continue\n",
    "            \n",
    "        # Find date clusters\n",
    "        date_clusters = []\n",
    "        used_bank_indices = set()\n",
    "        \n",
    "        for _, int_row in internal.iterrows():\n",
    "            int_date = int_row['Date_Match']\n",
    "            int_amount = int_row['Amount']\n",
    "            \n",
    "            # Find bank transactions within date tolerance\n",
    "            bank_matches = bank[\n",
    "                (abs((bank['Date_Match'] - int_date).dt.days <= date_tol) &\n",
    "                (~bank.index.isin(used_bank_indices)))\n",
    "            ]\n",
    "            \n",
    "            if not bank_matches.empty:\n",
    "                # Create cluster\n",
    "                cluster = {\n",
    "                    'sender': sender,\n",
    "                    'internal_ids': [int_row['ID']],\n",
    "                    'bank_ids': bank_matches.index.tolist(),\n",
    "                    'internal_amounts': [int_amount],\n",
    "                    'bank_amounts': bank_matches['Amount'].tolist(),\n",
    "                    'start_date': min(int_date, bank_matches['Date_Match'].min()),\n",
    "                    'end_date': max(int_date, bank_matches['Date_Match'].max())\n",
    "                }\n",
    "                \n",
    "                # Mark bank transactions as used\n",
    "                used_bank_indices.update(bank_matches.index)\n",
    "                date_clusters.append(cluster)\n",
    "        \n",
    "        # Analyze clusters\n",
    "        for cluster in date_clusters:\n",
    "            total_internal = sum(cluster['internal_amounts'])\n",
    "            total_bank = sum(cluster['bank_amounts'])\n",
    "            discrepancy = total_internal - total_bank\n",
    "            \n",
    "            clusters.append({\n",
    "                'Sender': sender,\n",
    "                'Date Range': f\"{cluster['start_date'].date()} to {cluster['end_date'].date()}\",\n",
    "                'Internal Transactions': len(cluster['internal_amounts']),\n",
    "                'Bank Transactions': len(cluster['bank_amounts']),\n",
    "                'Total Internal': total_internal,\n",
    "                'Total Bank': total_bank,\n",
    "                'Discrepancy': discrepancy\n",
    "            })\n",
    "            \n",
    "            if abs(discrepancy) > 1:  # Ignore tiny rounding differences\n",
    "                discrepancies.append({\n",
    "                    'Sender': sender,\n",
    "                    'Period': f\"{cluster['start_date'].date()} to {cluster['end_date'].date()}\",\n",
    "                    'Internal Total': total_internal,\n",
    "                    'Bank Total': total_bank,\n",
    "                    'Difference': discrepancy,\n",
    "                    'Type': 'Under' if discrepancy > 0 else 'Over'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(clusters), pd.DataFrame(discrepancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the main reconciliation section:\n",
    "if not MP_ng_bank_df_recon.empty:\n",
    "    print(\"\\nPerforming initial reconciliation...\")\n",
    "    \n",
    "    # Ensure Date_Match is datetime in both DataFrames\n",
    "    MP_ng_hex_df_recon['Date_Match'] = pd.to_datetime(MP_ng_hex_df_recon['Date_Match'])\n",
    "    MP_ng_bank_df_recon['Date_Match'] = pd.to_datetime(MP_ng_bank_df_recon['Date_Match'])\n",
    "    \n",
    "    # Round amounts for matching\n",
    "    MP_ng_hex_df_recon['Amount_Rounded'] = MP_ng_hex_df_recon['Amount'].round(2)\n",
    "    MP_ng_bank_df_recon['Amount_Rounded'] = MP_ng_bank_df_recon['Amount'].round(2)\n",
    "    \n",
    "    # Rest of your reconciliation code...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9781d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading 3 sheets from /Users/gracegitau/Downloads/#Recon May/NGN/MP Bank.xls...\n",
      "Columns from first sheet (Monnify_Account_Statement): ['AMOUNT', 'BALANCE_BEFORE', 'BALANCE_AFTER', 'REFERENCE', 'DATE', 'TRANSACTION_TYPE', 'NARRATION']\n",
      "Appended Monnify_Account_Statement 2 with shape (65536, 7)\n",
      "Appended Monnify_Account_Statement 3 with shape (33805, 7)\n",
      "Combined shape: (164876, 7)\n",
      "\n",
      "Preprocessing internal records...\n",
      "\n",
      "Found 2 duplicate internal records:\n",
      "                                         ID       Amount       Date  \\\n",
      "7   fx-transfer-2z36clGZ7MtbiYoLQTM77cK633P  429800000.0 2025-06-26   \n",
      "98  fx-transfer-2z39wSNPNxBrmMu8GYZiYeF5Bkd  625200000.0 2025-06-26   \n",
      "\n",
      "                                  Description  \n",
      "7   fx-deal-quote-2z2jye54GJcgyUcTAq4cKtJIWkz  \n",
      "98  fx-deal-quote-2z2jye54GJcgyUcTAq4cKtJIWkz  \n",
      "\n",
      "Starting bank preprocessing. Initial shape: (164876, 7)\n",
      "Final bank shape: (140, 6)\n",
      "\n",
      "Performing initial reconciliation...\n",
      "\n",
      "Analyzing unmatched transactions with clustering...\n",
      "No clusters found in unmatched transactions\n",
      "\n",
      "Final Reconciliation Summary:\n",
      "Total Internal Amount: 41,041,219,451.68\n",
      "Total Bank Amount: 40,365,339,018.68\n",
      "Initial Discrepancy: 675,880,433.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. Data Loading Functions\n",
    "def load_moniepoint_bank_statement(filepath):\n",
    "    \"\"\"Load and combine multi-sheet MoniePoint bank statement\"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(filepath, engine='xlrd')\n",
    "        sheet_names = xls.sheet_names\n",
    "        \n",
    "        if not sheet_names:\n",
    "            print(\"No sheets found in the Excel file.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"Loading {len(sheet_names)} sheets from {filepath}...\")\n",
    "\n",
    "        # Read first sheet to get columns\n",
    "        df_first_sheet = pd.read_excel(filepath, sheet_name=sheet_names[0], engine='xlrd', header=0)\n",
    "        columns = df_first_sheet.columns.tolist()\n",
    "        print(f\"Columns from first sheet ({sheet_names[0]}): {columns}\")\n",
    "\n",
    "        # Process all sheets\n",
    "        all_dfs = [df_first_sheet]\n",
    "        for sheet_name in sheet_names[1:]:\n",
    "            df_sheet = pd.read_excel(filepath, sheet_name=sheet_name, engine='xlrd', header=None)\n",
    "            if df_sheet.shape[1] == len(columns):\n",
    "                df_sheet.columns = columns\n",
    "                all_dfs.append(df_sheet)\n",
    "                print(f\"Appended {sheet_name} with shape {df_sheet.shape}\")\n",
    "            else:\n",
    "                print(f\"Warning: Skipping {sheet_name} - column mismatch\")\n",
    "\n",
    "        df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        print(f\"Combined shape: {df_combined.shape}\")\n",
    "        return df_combined\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading bank statement: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 2. Sender Identification Functions\n",
    "def extract_sender_name(narration):\n",
    "    \"\"\"Extract sender name from bank narration\"\"\"\n",
    "    narration = str(narration).lower()\n",
    "    if 'verto financial tech' in narration or 'paga' in narration:\n",
    "        return 'VertoFX, NGN a/c'\n",
    "    elif 'sendfirst' in narration or 'duplo ltd' in narration:\n",
    "        return 'Duplo, NGN a/c'\n",
    "    elif 'esca' in narration:\n",
    "        return 'Esca Nigeria, NGN a/c'\n",
    "    elif 'resrv' in narration:\n",
    "        return 'Resrv FX, NGN a/c'\n",
    "    elif 'waza' in narration:\n",
    "        return 'Waza, Nigeria, NGN a/c'\n",
    "    elif 'flutterwave' in narration:\n",
    "        return 'Flutterwave, NGN a/c'\n",
    "    elif 'inexass' in narration:\n",
    "        return 'AZA Finance, NGN a/c'\n",
    "    elif 'nala' in narration:\n",
    "        return 'Nala Payments'\n",
    "    elif 'south one' in narration:\n",
    "        return 'Southone NGN a/c'\n",
    "    elif 'titan-paystack' in narration or 'multigate' in narration:\n",
    "        return 'Multigate, NGN a/c'\n",
    "    elif 'zerozilo' in narration or 'silverfile' in narration or 'palm bills' in narration:\n",
    "        return 'Fincra, NGN a/c'\n",
    "    elif 'ift technologies' in narration or 'budpay' in narration or 'bud infrastructure' in narration:\n",
    "        return 'Torus Mara, NGN a/c'\n",
    "    elif 'starks associates limited' in narration or 'shamiri' in narration or 'second jeu' in narration:\n",
    "        return 'Straitpay (Starks), UK, NGN a/c'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def extract_internal_sender(description):\n",
    "    \"\"\"Extract sender name from internal description\"\"\"\n",
    "    description = str(description).lower()\n",
    "    if 'fx-deal' in description:\n",
    "        for sender in ['verto', 'duplo', 'esca', 'resrv', 'waza', 'flutterwave']:\n",
    "            if sender in description:\n",
    "                return f\"{sender.title()}, NGN a/c\"\n",
    "    elif 'payout' in description:\n",
    "        return 'Internal Payout'\n",
    "    return extract_sender_name(description)  # Fallback to bank patterns\n",
    "\n",
    "# 3. Bank Statement Preprocessing\n",
    "def preprocess_moniepoint_bank(df):\n",
    "    \"\"\"Clean and prepare bank statement data\"\"\"\n",
    "    print(f\"\\nStarting bank preprocessing. Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Clean columns\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert and filter dates\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'], errors='coerce')\n",
    "    df = df.dropna(subset=['DATE']).copy()\n",
    "    df = df[df['DATE'].dt.month == 6].copy()  # Filter for June\n",
    "    \n",
    "    # Apply business rules\n",
    "    df_cleaned = df[\n",
    "        (df['TRANSACTION_TYPE'] == 'CREDIT') & \n",
    "        (df['NARRATION'].str.contains('MFY-WT', na=False)) & \n",
    "        (~df['REFERENCE'].str.contains('RVSL', na=False))\n",
    "    ].copy()\n",
    "    \n",
    "    if df_cleaned.empty:\n",
    "        print(\"No records after filters\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract sender and combine transactions\n",
    "    df_cleaned['SENDER_NAME'] = df_cleaned['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    df_combined = df_cleaned.groupby(['SENDER_NAME', pd.Grouper(key='DATE', freq='30min')]).agg({\n",
    "        'AMOUNT': 'sum',\n",
    "        'NARRATION': lambda x: ' '.join(x.dropna().unique()),\n",
    "        'REFERENCE': lambda x: ' '.join(x.dropna().unique()),\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_combined.rename(columns={'DATE': 'DATE_TIME_WINDOW_START', 'AMOUNT': 'Amount'}, inplace=True)\n",
    "    df_combined['Date_Match'] = df_combined['DATE_TIME_WINDOW_START'].dt.date\n",
    "    print(f\"Final bank shape: {df_combined.shape}\")\n",
    "    return df_combined\n",
    "\n",
    "# 4. Clustering for Unmatched Transactions\n",
    "def cluster_unmatched_transactions(unmatched_internal, unmatched_bank, date_tol=3):\n",
    "    \"\"\"Group unmatched transactions by sender and date proximity\"\"\"\n",
    "    # Ensure date columns are datetime\n",
    "    unmatched_internal['Date_Match'] = pd.to_datetime(unmatched_internal['Date_Match'])\n",
    "    unmatched_bank['Date_Match'] = pd.to_datetime(unmatched_bank['Date_Match'])\n",
    "    \n",
    "    # Add sender names\n",
    "    unmatched_internal['SENDER_NAME'] = unmatched_internal['Description'].apply(extract_internal_sender)\n",
    "    unmatched_bank['SENDER_NAME'] = unmatched_bank['NARRATION'].apply(extract_sender_name)\n",
    "    \n",
    "    clusters = []\n",
    "    discrepancies = []\n",
    "    \n",
    "    # Process by sender\n",
    "    for sender in set(unmatched_internal['SENDER_NAME'].unique()).union(set(unmatched_bank['SENDER_NAME'].unique())):\n",
    "        internal = unmatched_internal[unmatched_internal['SENDER_NAME'] == sender]\n",
    "        bank = unmatched_bank[unmatched_bank['SENDER_NAME'] == sender]\n",
    "        \n",
    "        if internal.empty and bank.empty:\n",
    "            continue\n",
    "            \n",
    "        # Find date clusters\n",
    "        date_clusters = []\n",
    "        used_bank_indices = set()\n",
    "        \n",
    "        for _, int_row in internal.iterrows():\n",
    "            int_date = int_row['Date_Match']\n",
    "            int_amount = int_row['Amount']\n",
    "            \n",
    "            # Find bank transactions within date tolerance\n",
    "            bank_matches = bank[\n",
    "                (abs((bank['Date_Match'] - int_date).dt.days <= date_tol) &\n",
    "                (~bank.index.isin(used_bank_indices)))\n",
    "            ]\n",
    "            \n",
    "            if not bank_matches.empty:\n",
    "                # Create cluster\n",
    "                cluster = {\n",
    "                    'sender': sender,\n",
    "                    'internal_ids': [int_row['ID']],\n",
    "                    'bank_ids': bank_matches.index.tolist(),\n",
    "                    'internal_amounts': [int_amount],\n",
    "                    'bank_amounts': bank_matches['Amount'].tolist(),\n",
    "                    'start_date': min(int_date, bank_matches['Date_Match'].min()),\n",
    "                    'end_date': max(int_date, bank_matches['Date_Match'].max())\n",
    "                }\n",
    "                \n",
    "                # Mark bank transactions as used\n",
    "                used_bank_indices.update(bank_matches.index)\n",
    "                date_clusters.append(cluster)\n",
    "        \n",
    "        # Analyze clusters\n",
    "        for cluster in date_clusters:\n",
    "            total_internal = sum(cluster['internal_amounts'])\n",
    "            total_bank = sum(cluster['bank_amounts'])\n",
    "            discrepancy = total_internal - total_bank\n",
    "            \n",
    "            clusters.append({\n",
    "                'Sender': sender,\n",
    "                'Date Range': f\"{cluster['start_date'].date()} to {cluster['end_date'].date()}\",\n",
    "                'Internal Transactions': len(cluster['internal_amounts']),\n",
    "                'Bank Transactions': len(cluster['bank_amounts']),\n",
    "                'Total Internal': total_internal,\n",
    "                'Total Bank': total_bank,\n",
    "                'Discrepancy': discrepancy\n",
    "            })\n",
    "            \n",
    "            if abs(discrepancy) > 1:  # Ignore tiny rounding differences\n",
    "                discrepancies.append({\n",
    "                    'Sender': sender,\n",
    "                    'Period': f\"{cluster['start_date'].date()} to {cluster['end_date'].date()}\",\n",
    "                    'Internal Total': total_internal,\n",
    "                    'Bank Total': total_bank,\n",
    "                    'Difference': discrepancy,\n",
    "                    'Type': 'Under' if discrepancy > 0 else 'Over'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(clusters), pd.DataFrame(discrepancies)\n",
    "\n",
    "# 5. Visualization Functions\n",
    "def visualize_discrepancies(discrepancies_df):\n",
    "    \"\"\"Generate plots for discrepancy analysis\"\"\"\n",
    "    if discrepancies_df.empty:\n",
    "        print(\"No discrepancies to visualize\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Under/over payment by sender\n",
    "    plt.subplot(1, 2, 1)\n",
    "    discrepancies_df.groupby(['Sender', 'Type'])['Difference'].sum().unstack().plot(\n",
    "        kind='bar', stacked=True, color=['red', 'green']\n",
    "    )\n",
    "    plt.title(\"Discrepancies by Sender\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Time trend\n",
    "    plt.subplot(1, 2, 2)\n",
    "    discrepancies_df['Period'] = discrepancies_df['Period'].str.split(' to ').str[0]\n",
    "    discrepancies_df['Period'] = pd.to_datetime(discrepancies_df['Period'])\n",
    "    discrepancies_df.groupby(['Period', 'Type'])['Difference'].sum().unstack().plot(\n",
    "        kind='line', marker='o', color=['red', 'green']\n",
    "    )\n",
    "    plt.title(\"Discrepancies Over Time\")\n",
    "    plt.ylabel(\"Amount\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    MP_ng_hex_df = pd.read_csv('/Users/gracegitau/Downloads/#Recon May/NGN/MP Hex.csv')\n",
    "    MP_ng_bank_df = load_moniepoint_bank_statement('/Users/gracegitau/Downloads/#Recon May/NGN/MP Bank.xls')\n",
    "    \n",
    "    # 2. Preprocess Internal Records\n",
    "    print(\"\\nPreprocessing internal records...\")\n",
    "    MP_ng_hex_df.columns = MP_ng_hex_df.columns.str.strip()\n",
    "    MP_ng_hex_df = MP_ng_hex_df.rename(columns={\n",
    "        'TRANSFER_DATE': 'Date',\n",
    "        'AMOUNT': 'Amount',\n",
    "        'COMMENT': 'Description',\n",
    "        'TRANSFER_ID': 'ID'\n",
    "    })\n",
    "    \n",
    "    MP_ng_hex_df['Date'] = pd.to_datetime(MP_ng_hex_df['Date'])\n",
    "    MP_ng_hex_df_recon = MP_ng_hex_df[MP_ng_hex_df['Amount'] > 0].copy()\n",
    "    MP_ng_hex_df_recon = MP_ng_hex_df_recon[['Date', 'Amount', 'Description', 'ID']].copy()\n",
    "    MP_ng_hex_df_recon['Date_Match'] = MP_ng_hex_df_recon['Date'].dt.date\n",
    "    \n",
    "    # Check duplicates\n",
    "    duplicates = MP_ng_hex_df_recon[MP_ng_hex_df_recon.duplicated(subset=['Description'], keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        print(f\"\\nFound {len(duplicates)} duplicate internal records:\")\n",
    "        print(duplicates[['ID', 'Amount', 'Date', 'Description']].head())\n",
    "    \n",
    "    # 3. Preprocess Bank Statements\n",
    "    if not MP_ng_bank_df.empty:\n",
    "        MP_ng_bank_df_recon = preprocess_moniepoint_bank(MP_ng_bank_df)\n",
    "    else:\n",
    "        MP_ng_bank_df_recon = pd.DataFrame()\n",
    "        print(\"No bank data loaded\")\n",
    "    \n",
    "    # 4. Initial Reconciliation\n",
    "    if not MP_ng_bank_df_recon.empty:\n",
    "        print(\"\\nPerforming initial reconciliation...\")\n",
    "        # Round amounts for matching\n",
    "        MP_ng_hex_df_recon['Amount_Rounded'] = MP_ng_hex_df_recon['Amount'].round(2)\n",
    "        MP_ng_bank_df_recon['Amount_Rounded'] = MP_ng_bank_df_recon['Amount'].round(2)\n",
    "        \n",
    "        # Exact matches\n",
    "        reconciled_df = pd.merge(\n",
    "            MP_ng_hex_df_recon.assign(Source='Internal'),\n",
    "            MP_ng_bank_df_recon.assign(Source='Bank'),\n",
    "            on=['Date_Match', 'Amount_Rounded'],\n",
    "            how='outer',\n",
    "            suffixes=('_Internal', '_Bank')\n",
    "        )\n",
    "        \n",
    "        matched = reconciled_df.dropna(subset=['Source_Internal', 'Source_Bank'])\n",
    "        unmatched_internal = reconciled_df[reconciled_df['Source_Bank'].isna()].copy()\n",
    "        unmatched_bank = reconciled_df[reconciled_df['Source_Internal'].isna()].copy()\n",
    "        \n",
    "        # Prepare columns for clustering\n",
    "        unmatched_internal = unmatched_internal.rename(columns={\n",
    "            'Date_Internal': 'Date',\n",
    "            'Amount_Internal': 'Amount',\n",
    "            'Description_Internal': 'Description',\n",
    "            'ID_Internal': 'ID',\n",
    "            'Date_Match_Internal': 'Date_Match'\n",
    "        })[['Date', 'Amount', 'Description', 'ID', 'Date_Match']]\n",
    "        \n",
    "        unmatched_bank = unmatched_bank.rename(columns={\n",
    "            'DATE_TIME_WINDOW_START_Bank': 'DATE_TIME_WINDOW_START',\n",
    "            'Amount_Bank': 'Amount',\n",
    "            'NARRATION_Bank': 'NARRATION',\n",
    "            'SENDER_NAME_Bank': 'SENDER_NAME',\n",
    "            'Date_Match_Bank': 'Date_Match'\n",
    "        })[['DATE_TIME_WINDOW_START', 'Amount', 'NARRATION', 'SENDER_NAME', 'Date_Match']]\n",
    "        \n",
    "        # 5. Cluster Analysis for Unmatched\n",
    "        if not unmatched_internal.empty and not unmatched_bank.empty:\n",
    "            print(\"\\nAnalyzing unmatched transactions with clustering...\")\n",
    "            clusters_df, discrepancies_df = cluster_unmatched_transactions(\n",
    "                unmatched_internal,\n",
    "                unmatched_bank\n",
    "            )\n",
    "            \n",
    "            if not clusters_df.empty:\n",
    "                print(\"\\nTransaction Clusters Summary:\")\n",
    "                print(clusters_df.to_string())\n",
    "                \n",
    "                if not discrepancies_df.empty:\n",
    "                    print(\"\\nDiscrepancies Found:\")\n",
    "                    print(discrepancies_df.to_string())\n",
    "                    \n",
    "                    # Calculate totals\n",
    "                    under = discrepancies_df[discrepancies_df['Type'] == 'Under']['Difference'].sum()\n",
    "                    over = discrepancies_df[discrepancies_df['Type'] == 'Over']['Difference'].sum()\n",
    "                    \n",
    "                    print(f\"\\nSummary:\")\n",
    "                    print(f\"Total Under-Payments: {under:,.2f}\")\n",
    "                    print(f\"Total Over-Payments: {abs(over):,.2f}\")\n",
    "                    print(f\"Net Discrepancy: {under + over:,.2f}\")\n",
    "                    \n",
    "                    # Visualize\n",
    "                    visualize_discrepancies(discrepancies_df)\n",
    "            else:\n",
    "                print(\"No clusters found in unmatched transactions\")\n",
    "        \n",
    "        # 6. Final Reporting\n",
    "        total_internal = MP_ng_hex_df_recon['Amount'].sum()\n",
    "        total_bank = MP_ng_bank_df_recon['Amount'].sum()\n",
    "        initial_discrepancy = total_internal - total_bank\n",
    "        \n",
    "        print(\"\\nFinal Reconciliation Summary:\")\n",
    "        print(f\"Total Internal Amount: {total_internal:,.2f}\")\n",
    "        print(f\"Total Bank Amount: {total_bank:,.2f}\")\n",
    "        print(f\"Initial Discrepancy: {initial_discrepancy:,.2f}\")\n",
    "        \n",
    "        if 'discrepancies_df' in locals() and not discrepancies_df.empty:\n",
    "            cluster_discrepancy = under + over\n",
    "            print(f\"Discrepancy from Clusters: {cluster_discrepancy:,.2f}\")\n",
    "            print(f\"Unaccounted Difference: {initial_discrepancy - cluster_discrepancy:,.2f}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping reconciliation - no valid bank data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312612a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
